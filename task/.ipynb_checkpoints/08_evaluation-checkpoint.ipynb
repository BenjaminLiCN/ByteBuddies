{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35536f6-166c-4b89-8136-96417db5be30",
   "metadata": {
    "id": "b35536f6-166c-4b89-8136-96417db5be30"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 8 [评估]：** RAG 评估</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "欢迎学习本课程的最后一个 notebook！您已经在前面的 notebook 中将向量存储解决方案集成到 RAG 工作流中了！您将在本 notebook 采用相同的工作流，并通过 LLM-as-a-Judge 量化地评估 RAG！\n",
    "\n",
    "<br> \n",
    "\n",
    "### **学习目标：**\n",
    "\n",
    "* 了解如何集成之前 notebook 中的技术，量化 RAG 工作流的效果。\n",
    "* **最终练习**：***在课程环境中执行本 notebook，*您就能提交代码！**\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **思考问题：**\n",
    "\n",
    "* 在学习的过程中，请记住我们的指标代表的到底是什么。我们的工作流应该达到这些目标么？通过 LLM 进行评判是否足以评估工作流？具体的某个指标对我们的用例是否有意义？\n",
    "* 如果我们在链中保留 vectorstore-as-a-memory 组件，还能通过评估么？此外，评估是否有助于评估 vectorstore-as-a-memory 的性能？\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **Notebook 版权声明：**\n",
    "\n",
    "* 本 notebook 是 [**NVIDIA 深度学习培训中心**](https://www.nvidia.cn/training/)的课程[**《构建大语言模型 RAG 智能体》**](https://www.nvidia.cn/training/instructor-led-workshops/building-rag-agents-with-llms/)中的一部分，未经 NVIDIA 授权不得分发。\n",
    "\n",
    "<br> \n",
    "\n",
    "### **环境设置：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "w_A3rZOrIeQD",
   "metadata": {
    "id": "w_A3rZOrIeQD"
   },
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEgV11oZmJGg",
   "metadata": {
    "id": "zEgV11oZmJGg"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 1 部分：** 预发行评估\n",
    "\n",
    "在之前的 Notebook 中，我们成功地结合了多个概念创建文档聊天机器人，以实现高响应、有信息量的交互。然而，用户交互的多样性要求我们进行相对全面的测试，才能真正了解聊天机器人的性能。在不同场景下进行全面测试对于保障系统的功能、通用性及符合用户期望是至关重要的。\n",
    "\n",
    "在定义好聊天机器人的角色并实现了必要的功能之后，可以分多个阶段来进行评估：\n",
    "\n",
    "* **典型应用检测：**先测试与您的用例最贴近的场景。观察您的聊天机器人能否在有限的人工干预下进行可靠的对话。\n",
    "\n",
    "\t+ 此外，识别出应转给人工以检查/监督的边界或者分支情况（比如，换人工来确认交易或执行敏感操作），并执行。\n",
    "\n",
    "* **边界情况（Edge Case）检测：**探索典型场景的边界，确认聊天机器人如何处理不常见但合理的场景。\n",
    "\n",
    "\t+ 在任何公开发布之前，请评估可能构成责任风险的关键边界条件，比如生成不当内容的可能性。\n",
    "\t+ 在所有的输出（甚至是输入）上实现测试好的护栏（guardrails），以限制不良交互，并将用户引导到可预测的对话流上。\n",
    "\n",
    "* **渐进式推出（Progressive Rollout）：**向有限受众推出您的模型（先在内部推出，然后做 [A/B 测试](https://en.wikipedia.org/wiki/A/B_testing)）并实现分析功能，比如用量分析面板和反馈途径（投诉/喜欢/不喜欢/等等）。\n",
    "\n",
    "这三个步骤中，前两个可由一个小团队或个人完成，并在开发过程中持续迭代。不幸的是这需要频繁地进行，还容易发生人为错误。**幸运的是，我们可以借助 LLM-as-a-Judge 范式（formulation）！**\n",
    "\n",
    "*(是的，现在您可能已经不会惊讶了。就是因为 LLM 这么强，所以我们才专门做了这个课程)。*\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 2 部分：** LLM-as-a-Judge 范式\n",
    "\n",
    "在对话式 AI 领域，用 LLM 作为评估器或“评委”已经成为一种对自然语言任务表现进行可配置自动测试的方法了：\n",
    "\n",
    "* LLM 可以模拟一系列交互场景并生成合成数据，从而生成有针对性的输入来激发聊天机器人的一系列行为。\n",
    "* 聊天机器人在合成数据上的反应/检索可由 LLM 进行评估或解析，并且可以强制输出成“通过”/“失败”、相似程度或抽取等格式。\n",
    "* 许多此类结果都可以聚合成一个指标，按“通过评估的百分比”、“从数据源中检索到的相关信息量”、“余弦相似度均值”来解读。\n",
    "\n",
    "这种使用 LLM 测试和量化聊天机器人质量的想法称为 [\"LLM-as-a-Judge\"](https://arxiv.org/abs/2306.05685)，能进行与人类判断高度一致的测评，还能进行微调并大规模应用。\n",
    "\n",
    "**有几个现成的热门框架，包括：**\n",
    "* [**RAGAs (RAG Assessement)**](https://docs.ragas.io/en/stable/)，这是自行评估的一个很好的起点。\n",
    "* [**LangChain Evaluators**](https://python.langchain.com/docs/guides/evaluation/)，这是类似的第一方选项，具有许多可隐式构建的智能体。\n",
    "\n",
    "比起按原样使用链，我们会进行扩展，用一个更定制话的方案进行评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDDNaBA9N3XM",
   "metadata": {
    "id": "fDDNaBA9N3XM"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## **第 3 部分：** [评估准备] 成对数据评估器（Pairwise Evaluator）\n",
    "\n",
    "下面的练习将实现一个简化的 [LangChain 成对字符串评估器（Pairwise String Evaluator）](https://python.langchain.com/docs/guides/evaluation/examples/comparisons)。\n",
    "\n",
    "**为评估 RAG 链做准备，我们需要：**\n",
    "\n",
    "* 拉取文档索引（我们在上一个 notebook 中保存的）。\n",
    "* 重构我们的 RAG 工作流。\n",
    "\n",
    "**具体来说，我们将通过以下步骤实现评判范式：**\n",
    "\n",
    "* 对 RAG 文档池采样，拿到两个文档块。\n",
    "* 用这两个文档块生成一个合成的“基准”问答对。\n",
    "* 用 RAG 智能体生成它自己的答案。\n",
    "* 使用评委 LLM 比较这两种响应，其中，将生成的合成结果作为“标准答案”（ground-truth correct）。\n",
    "\n",
    "**该链应该执行一个简单但功能强大的过程，可基于以下目标进行测试：**\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***我的 RAG 链性能是否优于文档访问受限的聊天机器人。***\n",
    "\n",
    "**这就是要用来做最终评估的系统！**要是想了解一下这个系统是怎么集成到 Autograder 中的，可以看看 [`frontend/frontend_server.py`](frontend/frontend_server.py) 里的实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh8jaOqak0f",
   "metadata": {
    "id": "1bh8jaOqak0f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **任务 1：** 载入文档索引\n",
    "\n",
    "在本练习中，您将载入之前 notebook 创建的 `docstore_index` 文件。下面这个单元应该能把它按原样加载到存储中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tlE7a2lseLOy",
   "metadata": {
    "id": "tlE7a2lseLOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">394</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m394\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n",
      "\n",
      "Summary: Evaluating large language model (LLM) based chat assistants is challenging\n",
      "due to their broad capabilities and the inadequacy of existing benchmarks in\n",
      "measuring human preferences. To address this, we explore using strong LLMs as\n",
      "judges to evaluate these models on more open-ended questions. We examine the\n",
      "usage and limitations of LLM-as-a-judge, including position, verbosity, and\n",
      "self-enhancement biases, as well as limited reasoning ability, and propose\n",
      "solutions to mitigate some of them. We then verify the agreement between LLM\n",
      "judges and human preferences by introducing two benchmarks: MT-bench, a\n",
      "multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\n",
      "results reveal that strong LLM judges like GPT-4 can match both controlled and\n",
      "crowdsourced human preferences well, achieving over 80% agreement, the same\n",
      "level of agreement between humans. Hence, LLM-as-a-judge is a scalable and\n",
      "explainable way to approximate human preferences, which are otherwise very\n",
      "expensive to obtain. Additionally, we show our benchmark and traditional\n",
      "benchmarks complement each other by evaluating several variants of LLaMA and\n",
      "Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\n",
      "human preferences are publicly available at\n",
      "https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\n",
      "\n",
      "Page Body: .com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\\n1\\nIntroduction\\nThere has been a proliferation of LLM-based chat assistants (chatbots) that leverage supervised\\ninstruction fine-tuning and reinforcement learning with human feedback (RLHF) to unlock new\\ninstruction following and conversational abilities [31, 2, 30, 8, 52, 48, 14]. Once aligned with\\nhumans, these chat models are strongly preferred by human users over the original, unaligned models\\non which they are built. However, the heightened user preference does not always correspond to\\nimproved scores on traditional LLM benchmarks \\u2013 benchmarks like MMLU [19] and HELM [24]\\ncannot effectively tell the difference between these aligned models and the base models. This\\nphenomenon suggests that there is a fundamental discrepancy between user perceptions of the\\nusefulness of chatbots and the criteria adopted by conventional benchmarks\n"
     ]
    }
   ],
   "source": [
    "## Make sure you have docstore_index.tgz in your working directory\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "!tar xzvf docstore_index.tgz\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "## This printout just confirms that your store has been retrieved\n",
    "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "pprint(f\"Sample Chunk:\")\n",
    "print(format_chunk(docs[len(docs)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dib0F-t2N4LJ",
   "metadata": {
    "id": "dib0F-t2N4LJ"
   },
   "source": [
    "<br>  \n",
    "\n",
    "### **任务 2：[练习]** 载入 RAG 链\n",
    "\n",
    "现在我们有了索引，可以重新创建上一个 notebook 里的 RAG 智能体了！\n",
    "\n",
    "**主要的调整：**\n",
    "* 为了简单起见，您可以去掉 vectorstore-as-a-memory 组件。加上它会增加开销，还会让练习变复杂。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "XBi6Y8b8aXd2",
   "metadata": {
    "id": "XBi6Y8b8aXd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to know something interesting? Did you know that the largest snowflake ever recorded was 15 inches wide and 8 inches thick? It fell in Montana in 1887! Isn't that mind-blowing?\n",
      "\n",
      "( Source: Document Retrieval from various sources )"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    ")\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = [inputs]\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "#####################################################################\n",
    "## TODO: Pull in your desired RAG Chain. Memory not necessary\n",
    "\n",
    "## Chain 1 Specs: \"Hello World\" -> retrieval_chain \n",
    "##   -> {'input': <str>, 'context' : <str>}\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
    "context_getter = itemgetter('input') | docstore.as_retriever() | long_reorder | docs2str\n",
    "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
    "\n",
    "## Chain 2 Specs: retrieval_chain -> generator_chain \n",
    "##   -> {\"output\" : <str>, ...} -> output_puller\n",
    "generator_chain = chat_prompt | llm\n",
    "generator_chain = {'output' : generator_chain} | RunnableLambda(output_puller)  ## GIVEN\n",
    "\n",
    "## END TODO\n",
    "#####################################################################\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
    "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "### **第 3 步：** 生成合成问答对\n",
    "\n",
    "在本节中，我们可以实现评估流程的前两个步骤：\n",
    "\n",
    "* **对 RAG 文档池采样，拿到两个文档块。**\n",
    "* **用这两个文档块生成一个合成的“基准”问答对。**\n",
    "* 用 RAG 智能体生成它自己的答案。\n",
    "* 使用评委 LLM 比较这两种响应，其中，将生成的合成结果作为“标准答案”（ground-truth correct）。\n",
    "\n",
    "该链应该执行一个简单但功能强大的过程，可基于以下目标进行测试：\n",
    "\n",
    "> 我的 RAG 链性能是否优于文档访问受限的聊天机器人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ymzuX-DSNvL6",
   "metadata": {
    "id": "ymzuX-DSNvL6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: Can large language models (LLMs) such as RAG and GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> use their retrieval-augmented generation </span>\n",
       "<span style=\"font-weight: bold\">capabilities and LLM-as-a-judge role, respectively, to accurately judge and evaluate human-comprehensible language </span>\n",
       "<span style=\"font-weight: bold\">in various NLP tasks?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: Can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m such as RAG and GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m use their retrieval-augmented generation \u001b[0m\n",
       "\u001b[1mcapabilities and LLM-as-a-judge role, respectively, to accurately judge and evaluate human-comprehensible language \u001b[0m\n",
       "\u001b[1min various NLP tasks?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: Yes, and according to the papers, these LLMs can indeed match human preferences in evaluating language, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">with RAG models achieving over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% agreement with human preferences on three open-domain QA tasks and setting a new</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art, while GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> showing high agreement (over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">%) with human preferences in evaluating chatbots on </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the Chatbot Arena platform.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: Yes, and according to the papers, these LLMs can indeed match human preferences in evaluating language, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwith RAG models achieving over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% agreement with human preferences on three open-domain QA tasks and setting a new\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art, while GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m showing high agreement \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mover \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m%\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m with human preferences in evaluating chatbots on \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe Chatbot Arena platform.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How do pre-training methods that learn directly from raw text, such as those in NLP and visual models, </span>\n",
       "<span style=\"font-weight: bold\">enable zero-shot transfer to downstream tasks without the need for substantial task-specific architecture </span>\n",
       "<span style=\"font-weight: bold\">modifications or dataset-specific training data?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How do pre-training methods that learn directly from raw text, such as those in NLP and visual models, \u001b[0m\n",
       "\u001b[1menable zero-shot transfer to downstream tasks without the need for substantial task-specific architecture \u001b[0m\n",
       "\u001b[1mmodifications or dataset-specific training data?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: Pre-training methods, such as those described in BERT and CLIP, allow for the transfer of learned </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">representations to downstream tasks through the use of task-agnostic objectives and text-to-text interfaces, which </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">enable the model to learn a general and robust language or visual representation that can be adapted to various </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: Pre-training methods, such as those described in BERT and CLIP, allow for the transfer of learned \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrepresentations to downstream tasks through the use of task-agnostic objectives and text-to-text interfaces, which \u001b[0m\n",
       "\u001b[1;38;2;118;185;0menable the model to learn a general and robust language or visual representation that can be adapted to various \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: Can a model that combines pre-trained parametric and non-parametric memory for language generation </span>\n",
       "<span style=\"font-weight: bold\">accurately access and manipulate complex knowledge, and outperform traditional task-specific architectures on </span>\n",
       "<span style=\"font-weight: bold\">various NLP tasks?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: Can a model that combines pre-trained parametric and non-parametric memory for language generation \u001b[0m\n",
       "\u001b[1maccurately access and manipulate complex knowledge, and outperform traditional task-specific architectures on \u001b[0m\n",
       "\u001b[1mvarious NLP tasks?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: Yes, according to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, our </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">experiments demonstrate that retrieval-augmented generation (RAG) models, which combine pre-trained parametric and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">non-parametric memory, can achieve state-of-the-art results on a wide range of knowledge-intensive NLP tasks, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: Yes, according to the paper \u001b[0m\u001b[32m\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\u001b[0m\u001b[1;38;2;118;185;0m, our \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexperiments demonstrate that retrieval-augmented generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models, which combine pre-trained parametric and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnon-parametric memory, can achieve state-of-the-art results on a wide range of knowledge-intensive NLP tasks, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0moutperforming parametric seq2seq models and task-specific retrieve-and-extract architectures.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_questions = 3\n",
    "synth_questions = []\n",
    "synth_answers = []\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n",
    "\n",
    "for i in range(num_questions):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "        \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
    "    )\n",
    "    usr_msg = (\n",
    "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
    "        f\"Document2: {format_chunk(doc2)}\"\n",
    "    )\n",
    "\n",
    "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
    "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
    "    pprint2(f\"QA Pair {i+1}\")\n",
    "    pprint2(synth_questions[-1])\n",
    "    pprint(synth_answers[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5Q-3X4vS98P",
   "metadata": {
    "id": "c5Q-3X4vS98P"
   },
   "source": [
    "<br>  \n",
    "\n",
    "### **第 4 步：** 回答合成问题\n",
    "\n",
    "在本节中，我们可以实现评估流程的第三个步骤：\n",
    "\n",
    "* 对 RAG 文档池采样，拿到两个文档块。\n",
    "* 用这两个文档块生成一个合成的“基准”问答对。\n",
    "* **用 RAG 智能体生成它自己的答案。**\n",
    "* 使用评委 LLM 比较这两种响应，其中，将生成的合成结果作为“标准答案”（ground-truth correct）。\n",
    "\n",
    "该链应该执行一个简单但功能强大的过程，可基于以下目标进行测试：\n",
    "\n",
    "> 我的 RAG 链性能是否优于文档访问受限的聊天机器人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7T3GSwhZPHjF",
   "metadata": {
    "id": "7T3GSwhZPHjF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"font-weight: bold\">Question: Can large language models (LLMs) such as RAG and GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> use their retrieval-augmented generation </span>\n",
       "<span style=\"font-weight: bold\">capabilities and LLM-as-a-judge role, respectively, to accurately judge and evaluate human-comprehensible language </span>\n",
       "<span style=\"font-weight: bold\">in various NLP tasks?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[1mQuestion: Can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m such as RAG and GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m use their retrieval-augmented generation \u001b[0m\n",
       "\u001b[1mcapabilities and LLM-as-a-judge role, respectively, to accurately judge and evaluate human-comprehensible language \u001b[0m\n",
       "\u001b[1min various NLP tasks?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: To answer your question, let's take a look at what existing research has to say about the use of Large </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Language Models (LLMs) in evaluating human-comprehensible language. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">According to the study </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> strong LLMs like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> can indeed </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">match both controlled and crowdsourced human preferences well, achieving over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% agreement. This suggests that </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">LLMs, such as GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, can be used to accurately judge and evaluate human-comprehensible language in various NLP </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In fact, the authors of the study even propose that LLM-as-a-judge is a scalable and explainable way to approximate</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">human preferences, which are otherwise very expensive to obtain. They also created two benchmarks, MT-bench and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Chatbot Arena, to verify the agreement between LLM judges and human preferences. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">So, to answer your question directly, yes, it appears that LLMs like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> can use their retrieval-augmented </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generation capabilities and LLM-as-a-judge role to accurately judge and evaluate human-comprehensible language in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">various NLP tasks.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Would you like to know more about LLM-as-a-judge or how these models can be effectively used for evaluation?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: To answer your question, let's take a look at what existing research has to say about the use of Large \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLanguage Models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m in evaluating human-comprehensible language. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAccording to the study \u001b[0m\u001b[32m\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,\"\u001b[0m\u001b[1;38;2;118;185;0m strong LLMs like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m can indeed \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmatch both controlled and crowdsourced human preferences well, achieving over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% agreement. This suggests that \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLLMs, such as GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m, can be used to accurately judge and evaluate human-comprehensible language in various NLP \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn fact, the authors of the study even propose that LLM-as-a-judge is a scalable and explainable way to approximate\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhuman preferences, which are otherwise very expensive to obtain. They also created two benchmarks, MT-bench and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mChatbot Arena, to verify the agreement between LLM judges and human preferences. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSo, to answer your question directly, yes, it appears that LLMs like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m can use their retrieval-augmented \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgeneration capabilities and LLM-as-a-judge role to accurately judge and evaluate human-comprehensible language in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mvarious NLP tasks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mWould you like to know more about LLM-as-a-judge or how these models can be effectively used for evaluation?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">Question: How do pre-training methods that learn directly from raw text, such as those in NLP and visual models, </span>\n",
       "<span style=\"font-weight: bold\">enable zero-shot transfer to downstream tasks without the need for substantial task-specific architecture </span>\n",
       "<span style=\"font-weight: bold\">modifications or dataset-specific training data?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1mQuestion: How do pre-training methods that learn directly from raw text, such as those in NLP and visual models, \u001b[0m\n",
       "\u001b[1menable zero-shot transfer to downstream tasks without the need for substantial task-specific architecture \u001b[0m\n",
       "\u001b[1mmodifications or dataset-specific training data?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Pre-training methods that learn directly from raw text have indeed revolutionized NLP and visual </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">models. So, to answer your question, these pre-training methods enable zero-shot transfer by leveraging </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">task-agnostic objectives like autoregressive and masked language modeling. This allows them to scale across various</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">orders of magnitude in compute, model capacity, and data, steadily improving capabilities. For example, the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">development of </span><span style=\"color: #008000; text-decoration-color: #008000\">\"text-to-text\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> as a standardized input-output interface has enabled task-agnostic architectures to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">transfer directly to downstream datasets without the need for specialized output heads or dataset-specific </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">customization (McCann et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">; Radford et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2019</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">; Raffel et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2019</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">).</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Moreover, studies like the ones conducted on GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> (Radford et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2019</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) have focused on studying the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">task-learning capabilities of language models via zero-shot transfer. These studies have shown that pre-training </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">can indeed enable task learning as an </span><span style=\"color: #008000; text-decoration-color: #008000\">\"unexpected side-effect\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> (Liu et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). For instance, in the case of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">CLIP, which is pre-trained to predict if an image and a text snippet are paired together in its dataset, reuse this</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">capability to perform zero-shot classification without the need for substantial task-specific architecture </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">modifications or dataset-specific training data. This is made possible by the use of the cosine similarity of the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">feature embeddings of the image and the set of possible texts, scaled by a temperature parameter and normalized </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">into a probability distribution via a softmax function.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">It's worth noting that this approach has been benchmarked on over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> different existing computer vision datasets, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">classification. The results show that the model transfers non-trivially to most tasks and is often competitive with</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">a fully supervised baseline without the need for any dataset specific training. For instance, it was shown to match</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the accuracy of the original ResNet-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> on ImageNet without needing to use any of the </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.28</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> million training examples</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">it was trained on (Raffel et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2019</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">).</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Pre-training methods that learn directly from raw text have indeed revolutionized NLP and visual \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodels. So, to answer your question, these pre-training methods enable zero-shot transfer by leveraging \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtask-agnostic objectives like autoregressive and masked language modeling. This allows them to scale across various\u001b[0m\n",
       "\u001b[1;38;2;118;185;0morders of magnitude in compute, model capacity, and data, steadily improving capabilities. For example, the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdevelopment of \u001b[0m\u001b[32m\"text-to-text\"\u001b[0m\u001b[1;38;2;118;185;0m as a standardized input-output interface has enabled task-agnostic architectures to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtransfer directly to downstream datasets without the need for specialized output heads or dataset-specific \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcustomization \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMcCann et al., \u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;38;2;118;185;0m; Radford et al., \u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1;38;2;118;185;0m; Raffel et al., \u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mMoreover, studies like the ones conducted on GPT-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRadford et al., \u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m have focused on studying the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtask-learning capabilities of language models via zero-shot transfer. These studies have shown that pre-training \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcan indeed enable task learning as an \u001b[0m\u001b[32m\"unexpected side-effect\"\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLiu et al., \u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. For instance, in the case of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mCLIP, which is pre-trained to predict if an image and a text snippet are paired together in its dataset, reuse this\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcapability to perform zero-shot classification without the need for substantial task-specific architecture \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodifications or dataset-specific training data. This is made possible by the use of the cosine similarity of the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfeature embeddings of the image and the set of possible texts, scaled by a temperature parameter and normalized \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minto a probability distribution via a softmax function.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIt's worth noting that this approach has been benchmarked on over \u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;38;2;118;185;0m different existing computer vision datasets, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mspanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mclassification. The results show that the model transfers non-trivially to most tasks and is often competitive with\u001b[0m\n",
       "\u001b[1;38;2;118;185;0ma fully supervised baseline without the need for any dataset specific training. For instance, it was shown to match\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe accuracy of the original ResNet-\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;38;2;118;185;0m on ImageNet without needing to use any of the \u001b[0m\u001b[1;36m1.28\u001b[0m\u001b[1;38;2;118;185;0m million training examples\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mit was trained on \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRaffel et al., \u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "<span style=\"font-weight: bold\">Question: Can a model that combines pre-trained parametric and non-parametric memory for language generation </span>\n",
       "<span style=\"font-weight: bold\">accurately access and manipulate complex knowledge, and outperform traditional task-specific architectures on </span>\n",
       "<span style=\"font-weight: bold\">various NLP tasks?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\u001b[1mQuestion: Can a model that combines pre-trained parametric and non-parametric memory for language generation \u001b[0m\n",
       "\u001b[1maccurately access and manipulate complex knowledge, and outperform traditional task-specific architectures on \u001b[0m\n",
       "\u001b[1mvarious NLP tasks?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Based on the document </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the answer is yes. In fact, the authors found that their Retrieval-Augmented Generation (RAG) models, which combine</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">pre-trained parametric and non-parametric memory for language generation, can accurately access and manipulate </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">complex knowledge.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The RAG models use a pre-trained retriever to access a dense vector index of Wikipedia, allowing them to draw upon </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">a vast amount of external knowledge. By using this combination of parametric and non-parametric memory, the RAG </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">models are able to outperform traditional task-specific architectures on various NLP tasks, including open domain </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">question answering and trivia answering.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In particular, the authors found that their RAG models achieved state-of-the-art results on three open domain QA </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks: Open Natural Questions, WebQuestions, and CuratedTrec. They also found that their models strongly </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">outperformed recent approaches that use specialized pre-training objectives on TriviaQA.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">It's worth noting that the authors explored two different formulations of RAG, one that conditions on the same </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">retrieved passages across the whole generated sequence, and another that can use different passages per token. They</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">found that both formulations were effective, but the second was able to generate more specific, diverse, and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">factual language than the first.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, the results of this study suggest that combining pre-trained parametric and non-parametric memory for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language generation can lead to significant advances in NLP tasks that require access to complex knowledge.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Based on the document \u001b[0m\u001b[32m\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe answer is yes. In fact, the authors found that their Retrieval-Augmented Generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models, which combine\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpre-trained parametric and non-parametric memory for language generation, can accurately access and manipulate \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcomplex knowledge.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe RAG models use a pre-trained retriever to access a dense vector index of Wikipedia, allowing them to draw upon \u001b[0m\n",
       "\u001b[1;38;2;118;185;0ma vast amount of external knowledge. By using this combination of parametric and non-parametric memory, the RAG \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodels are able to outperform traditional task-specific architectures on various NLP tasks, including open domain \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mquestion answering and trivia answering.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn particular, the authors found that their RAG models achieved state-of-the-art results on three open domain QA \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks: Open Natural Questions, WebQuestions, and CuratedTrec. They also found that their models strongly \u001b[0m\n",
       "\u001b[1;38;2;118;185;0moutperformed recent approaches that use specialized pre-training objectives on TriviaQA.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIt's worth noting that the authors explored two different formulations of RAG, one that conditions on the same \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mretrieved passages across the whole generated sequence, and another that can use different passages per token. They\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfound that both formulations were effective, but the second was able to generate more specific, diverse, and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfactual language than the first.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOverall, the results of this study suggest that combining pre-trained parametric and non-parametric memory for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage generation can lead to significant advances in NLP tasks that require access to complex knowledge.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Generate some synthetic answers to the questions above.\n",
    "##   Try to use the same syntax as the cell above\n",
    "rag_answers = []\n",
    "for i, q in enumerate(synth_questions):\n",
    "    ## TODO: Compute the RAG Answer\n",
    "    rag_answer = \"\"\n",
    "    rag_answer = rag_chain.invoke(q)\n",
    "    rag_answers += [rag_answer]\n",
    "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
    "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho5cnN_Xt_yr",
   "metadata": {
    "id": "Ho5cnN_Xt_yr"
   },
   "source": [
    "<br>  \n",
    "\n",
    "### **第 5 步：** 实现人类偏好指标\n",
    "\n",
    "在本节中，我们可以实现评估流程的第四个步骤：\n",
    "\n",
    "* 对 RAG 文档池采样，拿到两个文档块。\n",
    "* 用这两个文档块生成一个合成的“基准”问答对。\n",
    "* 用 RAG 智能体生成它自己的答案。\n",
    "* **使用评委 LLM 比较这两种响应，其中，将生成的合成结果作为“标准答案”（ground-truth correct）。**\n",
    "\n",
    "该链应该执行一个简单但功能强大的过程，可基于以下目标进行测试：\n",
    "\n",
    "> 我的 RAG 链性能是否优于文档访问受限的聊天机器人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sf6f2oFLuPtu",
   "metadata": {
    "id": "sf6f2oFLuPtu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: Can large language models (LLMs) such as RAG and GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> use their retrieval-augmented generation</span>\n",
       "<span style=\"font-weight: bold\">capabilities and LLM-as-a-judge role, respectively, to accurately judge and evaluate human-comprehensible language </span>\n",
       "<span style=\"font-weight: bold\">in various NLP tasks?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: Can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m such as RAG and GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m use their retrieval-augmented generation\u001b[0m\n",
       "\u001b[1mcapabilities and LLM-as-a-judge role, respectively, to accurately judge and evaluate human-comprehensible language \u001b[0m\n",
       "\u001b[1min various NLP tasks?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: Yes, and according to the papers, these LLMs can indeed match human preferences in evaluating</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language, with RAG models achieving over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% agreement with human preferences on three open-domain QA tasks and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">setting a new state-of-the-art, while GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> showing high agreement (over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">%) with human preferences in evaluating </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">chatbots on the Chatbot Arena platform.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: Yes, and according to the papers, these LLMs can indeed match human preferences in evaluating\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage, with RAG models achieving over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% agreement with human preferences on three open-domain QA tasks and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msetting a new state-of-the-art, while GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m showing high agreement \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mover \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m%\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m with human preferences in evaluating \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mchatbots on the Chatbot Arena platform.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: To answer your question, let's take a look at what existing research has to say about the use of Large </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Language Models (LLMs) in evaluating human-comprehensible language. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">According to the study </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> strong LLMs like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> can indeed </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">match both controlled and crowdsourced human preferences well, achieving over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% agreement. This suggests that </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">LLMs, such as GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, can be used to accurately judge and evaluate human-comprehensible language in various NLP </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In fact, the authors of the study even propose that LLM-as-a-judge is a scalable and explainable way to approximate</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">human preferences, which are otherwise very expensive to obtain. They also created two benchmarks, MT-bench and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Chatbot Arena, to verify the agreement between LLM judges and human preferences. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">So, to answer your question directly, yes, it appears that LLMs like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> can use their retrieval-augmented </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generation capabilities and LLM-as-a-judge role to accurately judge and evaluate human-comprehensible language in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">various NLP tasks.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Would you like to know more about LLM-as-a-judge or how these models can be effectively used for evaluation?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: To answer your question, let's take a look at what existing research has to say about the use of Large \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLanguage Models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m in evaluating human-comprehensible language. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAccording to the study \u001b[0m\u001b[32m\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,\"\u001b[0m\u001b[1;38;2;118;185;0m strong LLMs like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m can indeed \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmatch both controlled and crowdsourced human preferences well, achieving over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% agreement. This suggests that \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLLMs, such as GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m, can be used to accurately judge and evaluate human-comprehensible language in various NLP \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn fact, the authors of the study even propose that LLM-as-a-judge is a scalable and explainable way to approximate\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhuman preferences, which are otherwise very expensive to obtain. They also created two benchmarks, MT-bench and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mChatbot Arena, to verify the agreement between LLM judges and human preferences. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSo, to answer your question directly, yes, it appears that LLMs like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m can use their retrieval-augmented \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgeneration capabilities and LLM-as-a-judge role to accurately judge and evaluate human-comprehensible language in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mvarious NLP tasks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mWould you like to know more about LLM-as-a-judge or how these models can be effectively used for evaluation?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">. Justification</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">The second answer introduces more evidence and specific research to support the ground truth answer, making it </span>\n",
       "<span style=\"font-weight: bold\">better and more comprehensive. The answer quotes a study, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,\"</span>\n",
       "<span style=\"font-weight: bold\">which provides an in-depth analysis of the LLM-as-a-judge concept and its potential applications. The study's </span>\n",
       "<span style=\"font-weight: bold\">findings, which show a high agreement (over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"font-weight: bold\">%) between GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> and human preferences, further strengthen the </span>\n",
       "<span style=\"font-weight: bold\">argument.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">The second answer is also more direct and explicit in its response to the question, providing a clear </span><span style=\"color: #008000; text-decoration-color: #008000\">\"yes\"</span><span style=\"font-weight: bold\"> answer </span>\n",
       "<span style=\"font-weight: bold\">and avoiding ambiguity. Additionally, the answer provides a suggestion for further discussion, which indicates that</span>\n",
       "<span style=\"font-weight: bold\">the author is willing to engage in a more in-depth conversation about the topic. This level of engagement and </span>\n",
       "<span style=\"font-weight: bold\">willingness to provide additional information is a key aspect of a high-quality answer.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">In contrast, the ground truth answer relies on the papers cited, which, while relevant, do not provide as much </span>\n",
       "<span style=\"font-weight: bold\">detail or context as the second answer. The ground truth answer also lacks a direct conclusion or explicit </span>\n",
       "<span style=\"font-weight: bold\">reference to the study, which could be seen as a minor drawback. </span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Overall, the second answer meets the evaluation criteria better than the ground truth answer.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m. Justification\u001b[0m\n",
       "\n",
       "\u001b[1mThe second answer introduces more evidence and specific research to support the ground truth answer, making it \u001b[0m\n",
       "\u001b[1mbetter and more comprehensive. The answer quotes a study, \u001b[0m\u001b[32m\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,\"\u001b[0m\n",
       "\u001b[1mwhich provides an in-depth analysis of the LLM-as-a-judge concept and its potential applications. The study's \u001b[0m\n",
       "\u001b[1mfindings, which show a high agreement \u001b[0m\u001b[1m(\u001b[0m\u001b[1mover \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1m%\u001b[0m\u001b[1m)\u001b[0m\u001b[1m between GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m and human preferences, further strengthen the \u001b[0m\n",
       "\u001b[1margument.\u001b[0m\n",
       "\n",
       "\u001b[1mThe second answer is also more direct and explicit in its response to the question, providing a clear \u001b[0m\u001b[32m\"yes\"\u001b[0m\u001b[1m answer \u001b[0m\n",
       "\u001b[1mand avoiding ambiguity. Additionally, the answer provides a suggestion for further discussion, which indicates that\u001b[0m\n",
       "\u001b[1mthe author is willing to engage in a more in-depth conversation about the topic. This level of engagement and \u001b[0m\n",
       "\u001b[1mwillingness to provide additional information is a key aspect of a high-quality answer.\u001b[0m\n",
       "\n",
       "\u001b[1mIn contrast, the ground truth answer relies on the papers cited, which, while relevant, do not provide as much \u001b[0m\n",
       "\u001b[1mdetail or context as the second answer. The ground truth answer also lacks a direct conclusion or explicit \u001b[0m\n",
       "\u001b[1mreference to the study, which could be seen as a minor drawback. \u001b[0m\n",
       "\n",
       "\u001b[1mOverall, the second answer meets the evaluation criteria better than the ground truth answer.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How do pre-training methods that learn directly from raw text, such as those in NLP and visual </span>\n",
       "<span style=\"font-weight: bold\">models, enable zero-shot transfer to downstream tasks without the need for substantial task-specific architecture </span>\n",
       "<span style=\"font-weight: bold\">modifications or dataset-specific training data?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How do pre-training methods that learn directly from raw text, such as those in NLP and visual \u001b[0m\n",
       "\u001b[1mmodels, enable zero-shot transfer to downstream tasks without the need for substantial task-specific architecture \u001b[0m\n",
       "\u001b[1mmodifications or dataset-specific training data?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: Pre-training methods, such as those described in BERT and CLIP, allow for the transfer of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">learned representations to downstream tasks through the use of task-agnostic objectives and text-to-text </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">interfaces, which enable the model to learn a general and robust language or visual representation that can be </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">adapted to various tasks.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: Pre-training methods, such as those described in BERT and CLIP, allow for the transfer of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlearned representations to downstream tasks through the use of task-agnostic objectives and text-to-text \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minterfaces, which enable the model to learn a general and robust language or visual representation that can be \u001b[0m\n",
       "\u001b[1;38;2;118;185;0madapted to various tasks.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Pre-training methods that learn directly from raw text have indeed revolutionized NLP and visual </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">models. So, to answer your question, these pre-training methods enable zero-shot transfer by leveraging </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">task-agnostic objectives like autoregressive and masked language modeling. This allows them to scale across various</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">orders of magnitude in compute, model capacity, and data, steadily improving capabilities. For example, the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">development of </span><span style=\"color: #008000; text-decoration-color: #008000\">\"text-to-text\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> as a standardized input-output interface has enabled task-agnostic architectures to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">transfer directly to downstream datasets without the need for specialized output heads or dataset-specific </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">customization (McCann et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">; Radford et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2019</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">; Raffel et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2019</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">).</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Moreover, studies like the ones conducted on GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> (Radford et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2019</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) have focused on studying the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">task-learning capabilities of language models via zero-shot transfer. These studies have shown that pre-training </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">can indeed enable task learning as an </span><span style=\"color: #008000; text-decoration-color: #008000\">\"unexpected side-effect\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> (Liu et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). For instance, in the case of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">CLIP, which is pre-trained to predict if an image and a text snippet are paired together in its dataset, reuse this</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">capability to perform zero-shot classification without the need for substantial task-specific architecture </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">modifications or dataset-specific training data. This is made possible by the use of the cosine similarity of the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">feature embeddings of the image and the set of possible texts, scaled by a temperature parameter and normalized </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">into a probability distribution via a softmax function.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">It's worth noting that this approach has been benchmarked on over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> different existing computer vision datasets, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">classification. The results show that the model transfers non-trivially to most tasks and is often competitive with</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">a fully supervised baseline without the need for any dataset specific training. For instance, it was shown to match</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the accuracy of the original ResNet-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> on ImageNet without needing to use any of the </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.28</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> million training examples</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">it was trained on (Raffel et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2019</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">).</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Pre-training methods that learn directly from raw text have indeed revolutionized NLP and visual \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodels. So, to answer your question, these pre-training methods enable zero-shot transfer by leveraging \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtask-agnostic objectives like autoregressive and masked language modeling. This allows them to scale across various\u001b[0m\n",
       "\u001b[1;38;2;118;185;0morders of magnitude in compute, model capacity, and data, steadily improving capabilities. For example, the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdevelopment of \u001b[0m\u001b[32m\"text-to-text\"\u001b[0m\u001b[1;38;2;118;185;0m as a standardized input-output interface has enabled task-agnostic architectures to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtransfer directly to downstream datasets without the need for specialized output heads or dataset-specific \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcustomization \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMcCann et al., \u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;38;2;118;185;0m; Radford et al., \u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1;38;2;118;185;0m; Raffel et al., \u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mMoreover, studies like the ones conducted on GPT-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRadford et al., \u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m have focused on studying the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtask-learning capabilities of language models via zero-shot transfer. These studies have shown that pre-training \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcan indeed enable task learning as an \u001b[0m\u001b[32m\"unexpected side-effect\"\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLiu et al., \u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. For instance, in the case of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mCLIP, which is pre-trained to predict if an image and a text snippet are paired together in its dataset, reuse this\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcapability to perform zero-shot classification without the need for substantial task-specific architecture \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodifications or dataset-specific training data. This is made possible by the use of the cosine similarity of the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfeature embeddings of the image and the set of possible texts, scaled by a temperature parameter and normalized \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minto a probability distribution via a softmax function.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIt's worth noting that this approach has been benchmarked on over \u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;38;2;118;185;0m different existing computer vision datasets, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mspanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mclassification. The results show that the model transfers non-trivially to most tasks and is often competitive with\u001b[0m\n",
       "\u001b[1;38;2;118;185;0ma fully supervised baseline without the need for any dataset specific training. For instance, it was shown to match\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe accuracy of the original ResNet-\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;38;2;118;185;0m on ImageNet without needing to use any of the \u001b[0m\u001b[1;36m1.28\u001b[0m\u001b[1;38;2;118;185;0m million training examples\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mit was trained on \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRaffel et al., \u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Justification: </span>\n",
       "<span style=\"font-weight: bold\">Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> is a better answer than Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> and does not introduce any inconsistencies. </span>\n",
       "\n",
       "<span style=\"font-weight: bold\">This answer provides more detailed insights and evidence from various studies and research papers, indicating that </span>\n",
       "<span style=\"font-weight: bold\">the pre-training methods indeed enable zero-shot transfer by leveraging task-agnostic objectives and standardized </span>\n",
       "<span style=\"font-weight: bold\">input-output interfaces. The answer also includes concrete examples, such as the use of CLIP for zero-shot </span>\n",
       "<span style=\"font-weight: bold\">classification, which demonstrates the effectiveness of these pre-training methods in achieving substantial </span>\n",
       "<span style=\"font-weight: bold\">transfer without the need for substantial task-specific architecture modifications or dataset-specific training </span>\n",
       "<span style=\"font-weight: bold\">data.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Moreover, the additional information about the benchmarking of this approach on over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span><span style=\"font-weight: bold\"> different existing computer</span>\n",
       "<span style=\"font-weight: bold\">vision datasets and its performance on various tasks provides further evidence and validation. This additional </span>\n",
       "<span style=\"font-weight: bold\">information is not included in Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> and adds to the validity and credibility of Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Therefore, Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> receives a score of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">, indicating that it is a better answer than the ground truth Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">, </span>\n",
       "<span style=\"font-weight: bold\">without introducing any inconsistencies.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\n",
       "\u001b[1mJustification: \u001b[0m\n",
       "\u001b[1mAnswer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m is a better answer than Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m and does not introduce any inconsistencies. \u001b[0m\n",
       "\n",
       "\u001b[1mThis answer provides more detailed insights and evidence from various studies and research papers, indicating that \u001b[0m\n",
       "\u001b[1mthe pre-training methods indeed enable zero-shot transfer by leveraging task-agnostic objectives and standardized \u001b[0m\n",
       "\u001b[1minput-output interfaces. The answer also includes concrete examples, such as the use of CLIP for zero-shot \u001b[0m\n",
       "\u001b[1mclassification, which demonstrates the effectiveness of these pre-training methods in achieving substantial \u001b[0m\n",
       "\u001b[1mtransfer without the need for substantial task-specific architecture modifications or dataset-specific training \u001b[0m\n",
       "\u001b[1mdata.\u001b[0m\n",
       "\n",
       "\u001b[1mMoreover, the additional information about the benchmarking of this approach on over \u001b[0m\u001b[1;36m30\u001b[0m\u001b[1m different existing computer\u001b[0m\n",
       "\u001b[1mvision datasets and its performance on various tasks provides further evidence and validation. This additional \u001b[0m\n",
       "\u001b[1minformation is not included in Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m and adds to the validity and credibility of Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m.\u001b[0m\n",
       "\n",
       "\u001b[1mTherefore, Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m receives a score of \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m, indicating that it is a better answer than the ground truth Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m, \u001b[0m\n",
       "\u001b[1mwithout introducing any inconsistencies.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: Can a model that combines pre-trained parametric and non-parametric memory for language </span>\n",
       "<span style=\"font-weight: bold\">generation accurately access and manipulate complex knowledge, and outperform traditional task-specific </span>\n",
       "<span style=\"font-weight: bold\">architectures on various NLP tasks?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: Can a model that combines pre-trained parametric and non-parametric memory for language \u001b[0m\n",
       "\u001b[1mgeneration accurately access and manipulate complex knowledge, and outperform traditional task-specific \u001b[0m\n",
       "\u001b[1marchitectures on various NLP tasks?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: Yes, according to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval-Augmented Generation for Knowledge-Intensive NLP </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Tasks\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, our experiments demonstrate that retrieval-augmented generation (RAG) models, which combine pre-trained </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">parametric and non-parametric memory, can achieve state-of-the-art results on a wide range of knowledge-intensive </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">NLP tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: Yes, according to the paper \u001b[0m\u001b[32m\"Retrieval-Augmented Generation for Knowledge-Intensive NLP \u001b[0m\n",
       "\u001b[32mTasks\"\u001b[0m\u001b[1;38;2;118;185;0m, our experiments demonstrate that retrieval-augmented generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models, which combine pre-trained \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mparametric and non-parametric memory, can achieve state-of-the-art results on a wide range of knowledge-intensive \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mNLP tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Based on the document </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the answer is yes. In fact, the authors found that their Retrieval-Augmented Generation (RAG) models, which combine</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">pre-trained parametric and non-parametric memory for language generation, can accurately access and manipulate </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">complex knowledge.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The RAG models use a pre-trained retriever to access a dense vector index of Wikipedia, allowing them to draw upon </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">a vast amount of external knowledge. By using this combination of parametric and non-parametric memory, the RAG </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">models are able to outperform traditional task-specific architectures on various NLP tasks, including open domain </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">question answering and trivia answering.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In particular, the authors found that their RAG models achieved state-of-the-art results on three open domain QA </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks: Open Natural Questions, WebQuestions, and CuratedTrec. They also found that their models strongly </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">outperformed recent approaches that use specialized pre-training objectives on TriviaQA.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">It's worth noting that the authors explored two different formulations of RAG, one that conditions on the same </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">retrieved passages across the whole generated sequence, and another that can use different passages per token. They</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">found that both formulations were effective, but the second was able to generate more specific, diverse, and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">factual language than the first.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, the results of this study suggest that combining pre-trained parametric and non-parametric memory for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language generation can lead to significant advances in NLP tasks that require access to complex knowledge.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Based on the document \u001b[0m\u001b[32m\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe answer is yes. In fact, the authors found that their Retrieval-Augmented Generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models, which combine\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpre-trained parametric and non-parametric memory for language generation, can accurately access and manipulate \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcomplex knowledge.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe RAG models use a pre-trained retriever to access a dense vector index of Wikipedia, allowing them to draw upon \u001b[0m\n",
       "\u001b[1;38;2;118;185;0ma vast amount of external knowledge. By using this combination of parametric and non-parametric memory, the RAG \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodels are able to outperform traditional task-specific architectures on various NLP tasks, including open domain \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mquestion answering and trivia answering.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn particular, the authors found that their RAG models achieved state-of-the-art results on three open domain QA \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks: Open Natural Questions, WebQuestions, and CuratedTrec. They also found that their models strongly \u001b[0m\n",
       "\u001b[1;38;2;118;185;0moutperformed recent approaches that use specialized pre-training objectives on TriviaQA.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIt's worth noting that the authors explored two different formulations of RAG, one that conditions on the same \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mretrieved passages across the whole generated sequence, and another that can use different passages per token. They\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfound that both formulations were effective, but the second was able to generate more specific, diverse, and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfactual language than the first.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOverall, the results of this study suggest that combining pre-trained parametric and non-parametric memory for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage generation can lead to significant advances in NLP tasks that require access to complex knowledge.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">. Justification: </span>\n",
       "<span style=\"font-weight: bold\">Although Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> provides more details and examples supporting the argument, some of the information mentioned is </span>\n",
       "<span style=\"font-weight: bold\">inconsistent with the ground truth (Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">). Specifically:</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">* Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> mentions that the RAG models outperform parametric seq2seq models and task-specific retrieve-and-extract</span>\n",
       "<span style=\"font-weight: bold\">architectures, but Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> only compares the RAG models to recent approaches that use specialized pre-training </span>\n",
       "<span style=\"font-weight: bold\">objectives on TriviaQA, and doesn't explicitly state this result.</span>\n",
       "<span style=\"font-weight: bold\">* Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> introduces a new finding regarding the comparison between the two RAG formulations, which is not </span>\n",
       "<span style=\"font-weight: bold\">mentioned in Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">. This inconsistency indicates that Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> may not be entirely consistent with the ground </span>\n",
       "<span style=\"font-weight: bold\">truth.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Additionally, some of the information in Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> is not present in Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">, such as the specific NLP tasks where </span>\n",
       "<span style=\"font-weight: bold\">the RAG models were tested (open domain question answering and trivia answering), which could indicate that Answer </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> is lying or attempts to provide more information that is not relevant to the question.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m. Justification: \u001b[0m\n",
       "\u001b[1mAlthough Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m provides more details and examples supporting the argument, some of the information mentioned is \u001b[0m\n",
       "\u001b[1minconsistent with the ground truth \u001b[0m\u001b[1m(\u001b[0m\u001b[1mAnswer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m. Specifically:\u001b[0m\n",
       "\n",
       "\u001b[1m* Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m mentions that the RAG models outperform parametric seq2seq models and task-specific retrieve-and-extract\u001b[0m\n",
       "\u001b[1marchitectures, but Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m only compares the RAG models to recent approaches that use specialized pre-training \u001b[0m\n",
       "\u001b[1mobjectives on TriviaQA, and doesn't explicitly state this result.\u001b[0m\n",
       "\u001b[1m* Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m introduces a new finding regarding the comparison between the two RAG formulations, which is not \u001b[0m\n",
       "\u001b[1mmentioned in Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m. This inconsistency indicates that Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m may not be entirely consistent with the ground \u001b[0m\n",
       "\u001b[1mtruth.\u001b[0m\n",
       "\n",
       "\u001b[1mAdditionally, some of the information in Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m is not present in Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m, such as the specific NLP tasks where \u001b[0m\n",
       "\u001b[1mthe RAG models were tested \u001b[0m\u001b[1m(\u001b[0m\u001b[1mopen domain question answering and trivia answering\u001b[0m\u001b[1m)\u001b[0m\u001b[1m, which could indicate that Answer \u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[1m is lying or attempts to provide more information that is not relevant to the question.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_prompt = ChatPromptTemplate.from_template(\"\"\"INSTRUCTION: \n",
    "Evaluate the following Question-Answer pair for human preference and consistency.\n",
    "Assume the first answer is a ground truth answer and has to be correct.\n",
    "Assume the second answer may or may not be true.\n",
    "[1] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
    "[2] The second answer is better than the first and does not introduce any inconsistencies.\n",
    "\n",
    "Output Format:\n",
    "[Score] Justification\n",
    "\n",
    "{qa_trio}\n",
    "\n",
    "EVALUATION: \n",
    "\"\"\")\n",
    "\n",
    "pref_score = []\n",
    "\n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
    "\n",
    "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
    "    pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
    "    pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
    "    pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
    "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "**恭喜！我们现在有了一个能评估我们工作流的 LLM 系统！**我们现在已经有了一些评判结果，可以简单地聚合出一个结果，看看在 LLM 的眼里我们的范式怎么样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3L_q6fMH3i6_",
   "metadata": {
    "id": "3L_q6fMH3i6_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "pref_score = sum((\"[2]\" in score) for score in pref_score) / len(pref_score)\n",
    "print(f\"Preference Score: {pref_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
   "metadata": {
    "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 4 部分：** 高级范式\n",
    "\n",
    "上述的练习都是在帮您为课程的最终评估做准备，展示了一个简单但有效的评估器链是怎么构建的。看过它的具体使用过程之后，您应该就能理解我们为您提供的目标和实现了。\n",
    "\n",
    "话说回来，这个指标只是帮我们回答以下问题的：\n",
    "* **哪种行为是对流水线很重要的？**\n",
    "* **要展示和评估这种行为，我们需要做些什么？**\n",
    "\n",
    "从这两个问题中，我们还能得出大量其它评估指标，这些指标可以评估不同的属性、整合不同的评估器链技术，甚至需要不同的流程组织策略。以下列出了一些其它常用的范式，尽管远算不上穷尽：\n",
    "\n",
    "* **风格评估（Style Evaluation）：**一些简单的评估范式可能就是想弄清楚：“我来提几个问题，看看输出像不像那么回事”。这样可以根据提供给评委 LLM 的描述来确定聊天机器人是否“表现出了应有的样子”。这种评估可以仅通过一些提示工程和 while 循环就实现出来。\n",
    "* **真值评估（Groud-Truth Evaluation）：**在我们的链中，我们通过采样合成生成了一些随机问题和答案，但实际上您可能已有一些有代表性的问题和答案，需要聊天机器人能始终如一地正确回答！在这种情况下，就需要对上面的练习链进行调整，并在开发过程中持续监控。\n",
    "* **检索/增强评估（Retrieval/Augmentation Evaluation）：**本课程对哪种预处理和提示步骤有利于工作流作了许多假设，其中大部分是通过实验确定的。文档预处理、分块策略、模型选择和提示词等因素都发挥着重要作用，因此创建验证这些决策的指标可能会有意义。着类指标可能需要您的工作流输出上下文块（context chunks），甚至完全依赖嵌入相似度来比较。尝试实现支持多个评估策略的链时，请记住这一点。可以把 [**RagasEvaluatorChain**](https://docs.ragas.io/en/latest/howtos/integrations/langchain.html) 抽象作为制定自定义通用评估流程的良好起点。\n",
    "* **轨迹评估（Trajectory Evaluation）：**使用更高级的智能体范式，您可以实现一个假设存在对话内存的多查询策略。借此，您可以实现一个能做到以下几点的评估智能体：\n",
    "\t+ 按顺序提出一系列问题，评估智能体在适应和迎合场景方面的能力。这种系统通常会考虑一系列对应关系，旨在评估智能体的对话“轨迹”。[**LangChain 轨迹评估文档**](https://python.langchain.com/docs/guides/evaluation/trajectory/)是一个很好的起点。\n",
    "\t+ 或者，您还可以实现一个评估智能体，通过与聊天机器人交互来进行评估。这样的智能体可以评估机器人是否能自然地将对话引导到问题的解决方案上，甚至可以用来生成性能的报告。[**LangChain 智能体文档**](https://python.langchain.com/docs/modules/agents/concepts) 是一个很好的起点！\n",
    "\n",
    "<br>\n",
    "\n",
    "最后，请务必恰当地使用工具。在课程的这个时候，您应该已经熟悉 LLM 的核心价值了：**它们功能强大、可扩展、可预测、可控且可编排 ...... 但默认情况下，它们的行为会变得不可预测。**评估您的需求，制定和验证您的工作流，尽可能多地进行控制，让您的系统能够稳定、高效地工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 5 部分：[评估]** 课程评估\n",
    "\n",
    "欢迎来到本课程的最后一个练习！希望您喜欢这个课程的内容，并准备好得分了！在这部分：\n",
    "\n",
    "* **确保您处于课程环境中**\n",
    "* **请确保 `docstore_index/` 已上传至课程环境，即完成 [`07_vectorstores.ipynb`](07_vectorstores.ipynb) 中的练习**\n",
    "\t+ **且至少包含一篇最近更新过的 [Arxiv 论文](https://arxiv.org/search/advanced)。**\n",
    "\t+ **确保没有已经在占用端口的 09_langserve.ipynb 会话。评估要求您实现新的 /retriever 和 /generator 入口！！**\n",
    "\n",
    "**目标：** \n",
    "\n",
    "启动前端服务 frontend 后，您在前端 Web 界面点击右下角的“Evaluate”便会触发 [`frontend/frontend_block.py`](frontend/frontend_block.py) 里的评估代码。您的目标就是借助我们在这门课程中构建的工作流，通过评估代码设置的通过条件！请将 [`09_langserve.ipynb`](09_langserve.ipynb) 作为参考！\n",
    "\n",
    "**提示：** \n",
    "- 可以参考 [`09_langserve.ipynb`](09_langserve.ipynb) 中已有的 `basic_chat` 服务，实现 `retriever` 和 `generator` 服务。\n",
    "- 为了达到最好的效果，“Main Route”应该选择“Basic”还是“RAG”？相信您现在可以自行做出判断了。\n",
    "\n",
    "**注意：**\n",
    "- 如果您的得分略小于通过评估所需的分数，有可能是语言模型在输出格式上表现不稳定导致的，可以尝试再次运行。\n",
    "\n",
    "现在，运行下方代码打开前端 Web 界面，点击右下角的“Evaluate”完成课程评估吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48e300ed-951c-4006-ac54-cbbd41251707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var url = 'http://'+window.location.host+':8090';\n",
       "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f4ed0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**得到通过评估的回复后**（“Congrats! You've passed the assessment!!”），不要停止课程环境，在浏览器中回到您启动课程环境的网页，单击“ASSESS TASK”按钮！\n",
    "\n",
    "> <img src=\"imgs/assess.png\" width=1200px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
   "metadata": {
    "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## <font color=\"#76b900\">**恭喜您完成了课程**</font>\n",
    "\n",
    "希望本课程既令人兴奋还具有一定的挑战性，能为您从事 LLM 和 RAG 系统开发的前沿工作做好充分的准备！现在，您应该已经具备了应对行业级挑战所需的技能，能用开源模型和框架部署 RAG 应用了。\n",
    "\n",
    "**您可能会感兴趣的一些 NVIDIA 产品：**\n",
    "* [**NVIDIA NIMs**](https://www.nvidia.com/en-us/ai/)，提供了可本地部署的微服务启动例程。\n",
    "* [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) 是目前推荐的，在生产环境部署 GPU 加速的 LLM 模型的引擎。\n",
    "* [**NVIDIA 的生成式 AI 示例库**](https://github.com/NVIDIA/GenerativeAIExamples)，包括了当前的官方微服务示例应用，并将持续发布新的生产工作流。\n",
    "* [**基于知识的聊天机器人技术简介**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief)，探讨了更多的 RAG 系统公开信息。\n",
    "\n",
    "**此外，您可能有兴趣深入探讨的一些重要主题：**\n",
    "* [**LlamaIndex**](https://www.llamaindex.ai/)，有强大的组件，可以增强和改进 LangChain RAG 功能。\n",
    "* [**LangSmith**](https://docs.smith.langchain.com/)，LangChain 提供的智能体生产化服务。\n",
    "* [**Gradio**](https://www.gradio.app/)，虽然在课程中有所提及，但还有更多值得探索的接口选项。可以从 [**HuggingFace Spaces**](https://huggingface.co/spaces) 来获取一些灵感。\n",
    "* [**LangGraph**](https://python.langchain.com/docs/langgraph/)，是一个基于图形的 LLM 编排框架，对于那些对[多智能体工作流](https://blog.langchain.dev/langgraph-multi-agent-workflows/)感兴趣的学员是个很好的起点。\n",
    "* [**DSPy**](https://github.com/stanfordnlp/dspy)，一个流工程框架（flow engineering framework），允许您根据性能结果优化 LLM 编排流程。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

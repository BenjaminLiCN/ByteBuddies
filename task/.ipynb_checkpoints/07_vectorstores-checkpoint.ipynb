{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qk4Uw_iSr3Mc",
   "metadata": {
    "id": "Qk4Uw_iSr3Mc"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 7:** 使用向量存储实现检索增强生成</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "我们在前面的 notebook 中了解并尝试了嵌入模型。讨论了它在长文档比较中的应用，并以它为主干实现了基于语义的比较。本 notebook 将把这个思路用到检索模型上，探索如何靠*向量存储*来构建自动保存和检索信息的聊天机器人系统。\n",
    "\n",
    "<br>\n",
    "\n",
    "### **学习目标：**\n",
    "\n",
    "* 理解语义相似度系统是怎么方便地实现检索的。\n",
    "* 学会将检索模块整合到聊天模型系统中，以创建检索增强生成（RAG）工作流，用于完成文档检索或对话内存缓冲等任务。\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **思考问题：**\n",
    "\n",
    "* 本 notebook 不会尝试加入层次化推理（hierachical reasoning）或非朴素（non-naive）的 RAG，如规划智能体（palnning agents）。想想需要如何调整才能让这些组件在 LCEL 链中运行。\n",
    "* 思考将向量存储方案用在规模化部署的最好时机是什么，以及什么时候需要用 GPU 进行优化。\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **Notebook 版权声明：**\n",
    "\n",
    "* 本 notebook 是 [**NVIDIA 深度学习培训中心**](https://www.nvidia.cn/training/)的课程[**《构建大语言模型 RAG 智能体》**](https://www.nvidia.cn/training/instructor-led-workshops/building-rag-agents-with-llms/)中的一部分，未经 NVIDIA 授权不得分发。\n",
    "\n",
    "<br> \n",
    "\n",
    "### **环境设置：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5XmeiiOWtuxC",
   "metadata": {
    "id": "5XmeiiOWtuxC"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "## ^^ Comment out if you want to see the pip install process\n",
    "\n",
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e37fe234-2bdb-4107-8483-efda9aa5e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf",
   "metadata": {
    "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## 第 1 部分：RAG 工作流概述\n",
    "\n",
    "此 notebook 将探索多个范式并给出参考代码，以帮助您开始使用最常见一些的检索增强工作流。具体来说将涵盖以下部分（每个部分各有侧重）：\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***适用于交互式对话的向量存储工作流：***\n",
    "* 为新对话生成语义嵌入。\n",
    "* 将消息正文添加到向量存储以供检索。\n",
    "* 在向量存储中查询相关消息填充到 LLM 上下文中。\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***处理任意文档的工作流：***\n",
    "* **将文档分快并处理成有用信息。**\n",
    "* 为每个**新文档块**生成语义嵌入。\n",
    "* 将**块正文（chunk bodies）**存到向量存储中以供检索。\n",
    "* 在向量存储中查询相关的**块**，用来填充 LLM 上下文。\n",
    "\t+ ***可选：*修改/合成结果以获得更好的 LLM 结果。**\n",
    "\n",
    "<br>\n",
    "\n",
    "> **适用于任意文档目录的扩展工作流：**\n",
    "* 将**每个文档**分为多个块并处理成有用的信息。\n",
    "* 为每个新文档块生成语义嵌入。\n",
    "* 将块正文存到**可扩展的向量数据库中以实现快速检索**。\n",
    "\t+ ***可选：*利用更大系统的层次化结构或元数据结构。**\n",
    "* 在**向量数据库**中查询相关的块来填充 LLM 上下文。\n",
    "\t+ *可选：*修改/合成结果以获得更好的 LLM 结果。\n",
    "\n",
    "<br>  \n",
    "\n",
    "与 RAG 相关的一些重要术语都可以在 [**LlamaIndex Concepts 页面**](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html) 查到，这是学习 LlamaIndex 加载和检索策略的很好的资源。我们强烈建议您在学习此 notebook 的过程中参考它，并鼓励您在课后试试 LlamaIndex 亲手体会它的优缺点！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437",
   "metadata": {
    "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/data_connection_langchain.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Retrieval | LangChain**🦜️🔗](https://python.langchain.com/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XaZ20XoeSTD-",
   "metadata": {
    "id": "XaZ20XoeSTD-"
   },
   "source": [
    "----\n",
    "\n",
    "<br>  \n",
    "\n",
    "## **第 2 部分：** 用于对话历史的 RAG\n",
    "\n",
    "在之前的探索中，我们深入研究了文档嵌入模型的功能，并用它来嵌入、存储和比较文本的语义向量表示。尽管我们可以动手将其扩展到向量存储领域，但如果用标准 API 配合框架的话，就能发现它已经替我们完成了很多繁重的工作！\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LRx0XUf_Sdxw",
   "metadata": {
    "id": "LRx0XUf_Sdxw"
   },
   "source": [
    "### **第 1 步：** 创建一段对话\n",
    "\n",
    "想象一段 Llama-13B 聊天智能体和一只名为 Beras 的熊之间的对话。这段对话包含了大量细节和潜在的分支，为我们的研究提供了丰富的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "IUfCuMkoShWI",
   "metadata": {
    "id": "IUfCuMkoShWI"
   },
   "outputs": [],
   "source": [
    "conversation = [  ## This conversation was generated partially by an AI system, and modified to exhibit desirable properties\n",
    "    \"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\",\n",
    "    \"[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America\",\n",
    "    \"[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\",\n",
    "    \"[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you!\"\n",
    "    \"[Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.\",\n",
    "    \"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching documentaries about them.\"\n",
    "    \"[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just curious, ya know!\",\n",
    "    \"[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tDL2tAo2Skh2",
   "metadata": {
    "id": "tDL2tAo2Skh2"
   },
   "source": [
    "仍然可以用上一个 notebook 的手动嵌入策略，但我们完全可以让向量数据库替我们做！\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5hIp943mSqGZ",
   "metadata": {
    "id": "5hIp943mSqGZ"
   },
   "source": [
    "### **第 2 步：** 构建向量存储检索器\n",
    "\n",
    "为了流程化对话中的相似性查询，我们可以使用向量存储来帮助我们追踪文本！**向量存储**（Vector Stores）或者叫向量存储系统，对嵌入/比较策略的大部分底层细节做了抽象，为加载和比较向量提供了一个简洁的接口。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pnaOBgexS-kp",
   "metadata": {
    "id": "pnaOBgexS-kp"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/vector_stores.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Vector Stores | LangChain**🦜️🔗](https://python.langchain.com/docs/modules/data_connection/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DwZUh6kgS5Ki",
   "metadata": {
    "id": "DwZUh6kgS5Ki"
   },
   "source": [
    "<br>\n",
    "\n",
    "除了借助 API 简化流程外，向量存储还在背后实现了连接器（connector）、集成（integration）和优化。我们将从 [**FAISS 向量存储**](https://python.langchain.com/docs/integrations/vectorstores/faiss)开始，它集成了兼容 LangChain 的嵌入模型 [**FAISS (Facebook AI Similarity Search)**](https://github.com/facebookresearch/faiss)，从而允许在本地实现快速可扩展的流程！\n",
    "\n",
    "\n",
    "**具体来说：**\n",
    "\n",
    "1. 我们可以通过 `from_texts` 构造器将对话输入到 [**FAISS 向量存储**](https://python.langchain.com/docs/integrations/vectorstores/faiss)。这样我们的对话数据和嵌入模型就会用来创建索引。\n",
    "2. 然后，这个向量存储就可以作为检索器，支持用 LangChain 运行时 API 来检索文档。\n",
    "\n",
    "以下内容展示了如何构建 FAISS 向量存储并使用 LangChain `vectorstore` API 将其作为检索器使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1kE2-ejoTKKU",
   "metadata": {
    "id": "1kE2-ejoTKKU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.2 ms, sys: 8.04 ms, total: 79.2 ms\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## Streamlined from_texts FAISS vectorstore construction from text list\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "retriever = convstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muN66v5PW5dW",
   "metadata": {
    "id": "muN66v5PW5dW"
   },
   "source": [
    "现在，检索器可以像任何其他可运行的 LangChain 一样用于查询向量存储中的某些相关文档：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "kNZJTnlEWVYh",
   "metadata": {
    "id": "kNZJTnlEWVYh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you![Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
       "\u001b[32mrocky mountains?\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
       "\u001b[32mand their significance!'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I hope you get to visit them someday, Beras! It would be a great adventure for \u001b[0m\n",
       "\u001b[32myou!\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Thank you for the suggestion! Ill definitely keep it in mind for the future.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
       "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
       "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "SE1eDZTEWScC",
   "metadata": {
    "id": "SE1eDZTEWScC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001b[0m\n",
       "\u001b[32macross North America'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
       "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
       "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
       "\u001b[32mrocky mountains?\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
       "\u001b[32mand their significance!'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtNCEXLYTVf4",
   "metadata": {
    "id": "mtNCEXLYTVf4"
   },
   "source": [
    "如我们所见，检索工具从我们的查询中找到了一些语义相关的文档。您可能会注意到，不是所有文档都有用或清晰。比如，如果不是出于上下文，检索询问*“您的姓名”*时把*“Beras”*检索出来可能不是个好事。提前考虑到潜在的问题并让 LLM 组件相互协同更有可能让 RAG 达到好的效果。\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZEDEzpqmTYMv",
   "metadata": {
    "id": "ZEDEzpqmTYMv"
   },
   "source": [
    "### **第 3 步：** 将对话检索功能整合到我们的链中\n",
    "\n",
    "现在，我们已把检索器组件作为一个链了，可以像以前一样将其整合到现有的聊天系统中。具体来说，我们现在可以构建一个***保持在线（always-on）的 RAG*** 了，其中：\n",
    "* **默认情况下，检索器始终在检索上下文。**\n",
    "* **生成器根据检索到的上下文执行操作。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64abe478-9bcb-4802-a26e-dc5a1756e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        if preface: print(preface, end=\"\")\n",
    "        pprint(x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## Optional; Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "uue5UY3_TcvF",
   "metadata": {
    "id": "uue5UY3_TcvF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on the context provided, Beras lives in the Arctic. It seems like the Rocky Mountains are a foreign and warm </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">place compared to their usual environment.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mBased on the context provided, Beras lives in the Arctic. It seems like the Rocky Mountains are a foreign and warm \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mplace compared to their usual environment.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {question}\"\n",
    "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'question': (lambda x:x)\n",
    "    }\n",
    "    | context_prompt\n",
    "    # | RPrint()\n",
    "    | instruct_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(chain.invoke(\"Where does Beras live?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FSIqTMuuTjIh",
   "metadata": {
    "id": "FSIqTMuuTjIh"
   },
   "source": [
    "多试几个调用，看看新配置的效果。无论您选择的是哪个模型，都可以先从下面的几个问题开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4jDJwrYpTmpd",
   "metadata": {
    "id": "4jDJwrYpTmpd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that span across North America. Unfortunately, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">without more context, I can't give you a precise location beyond that. However, you can find out more by doing some</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">research online or watching documentaries about them. I hope you enjoy learning about the Rocky Mountains!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that span across North America. Unfortunately, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwithout more context, I can't give you a precise location beyond that. However, you can find out more by doing some\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mresearch online or watching documentaries about them. I hope you enjoy learning about the Rocky Mountains!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "-artagLfTpBy",
   "metadata": {
    "id": "-artagLfTpBy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that stretch across North America. To answer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">your question about their location, they do not directly border California. In fact, they run from Canada all the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">way to New Mexico, passing through the states of Montana, Idaho, Wyoming, Colorado, and a small part of northern </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">New Mexico. I hope that helps clarify their location for you!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that stretch across North America. To answer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0myour question about their location, they do not directly border California. In fact, they run from Canada all the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mway to New Mexico, passing through the states of Montana, Idaho, Wyoming, Colorado, and a small part of northern \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mNew Mexico. I hope that helps clarify their location for you!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains? Are they close to California?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "GDgjdfdpTrV5",
   "metadata": {
    "id": "GDgjdfdpTrV5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hi there! Beras is a big blue bear and based on the context, we know that they live in the arctic. However, the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">context doesn't provide details about the exact location or the distance between the Arctic and the Rocky </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mountains. The Rocky Mountains stretch across North America, but without specific information, it's difficult to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">provide an exact distance.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mHi there! Beras is a big blue bear and based on the context, we know that they live in the arctic. However, the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcontext doesn't provide details about the exact location or the distance between the Arctic and the Rocky \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMountains. The Rocky Mountains stretch across North America, but without specific information, it's difficult to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprovide an exact distance.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"How far away is Beras from the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wp9-8CbT0L9",
   "metadata": {
    "id": "8wp9-8CbT0L9"
   },
   "source": [
    "<br>  \n",
    "\n",
    "您可能会注意到把这个保持在线（always-on）的检索节点放到循环里效果很不错，因为目前输入 LLM 的上下文仍然相对较小。有必要反复尝试嵌入大小、上下文限制等配置，来更好地预测模型表现，并衡量为提高性能值得做出何种努力。\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OnpOybOhUCTf",
   "metadata": {
    "id": "OnpOybOhUCTf"
   },
   "source": [
    "### **第 4 步：** 自动对话存储\n",
    "\n",
    "现在向量存储已经可以工作了，我们最后再做一个集成：加一个调用 `add_texts` 更新存储状态的运行时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "FsK6-AtRVdcZ",
   "metadata": {
    "id": "FsK6-AtRVdcZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying ice cream, it's essential to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">remember that it's a vast mountain range filled with breathtaking natural wonders. You may not find ice cream shops</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">as easily as you would in a city; however, the experience of being in the Rocky Mountains is unparalleled. Let's </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">continue to discuss the awe-inspiring features of the Rocky Mountains that you must witness!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mWhile I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying ice cream, it's essential to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mremember that it's a vast mountain range filled with breathtaking natural wonders. You may not find ice cream shops\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mas easily as you would in a city; however, the experience of being in the Rocky Mountains is unparalleled. Let's \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcontinue to discuss the awe-inspiring features of the Rocky Mountains that you must witness!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Well hello there Beras, it's not too hard to guess that you seem to have a fondness for ice cream! Although we were</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">discussing the magnificent Rocky Mountains, your enthusiasm for a scoop or two of this delightful treat was quite </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">evident. It's always delightful to enjoy a good meal amidst nature's wonders!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mWell hello there Beras, it's not too hard to guess that you seem to have a fondness for ice cream! Although we were\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdiscussing the magnificent Rocky Mountains, your enthusiasm for a scoop or two of this delightful treat was quite \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mevident. It's always delightful to enjoy a good meal amidst nature's wonders!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello Beras! I must admit, I made a little error there. I was so captivated by the mention of the Rocky Mountains </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that I assumed your fondness for ice cream. However, I'm now delighted to learn that your favorite food is actually</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">honey! It's quite fitting for a big blue bear like you. The Rocky Mountains indeed offer a magnificent view, and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">I'm sure they would be even more enjoyable with a taste of your favorite food, don't you think?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mHello Beras! I must admit, I made a little error there. I was so captivated by the mention of the Rocky Mountains \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthat I assumed your fondness for ice cream. However, I'm now delighted to learn that your favorite food is actually\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhoney! It's quite fitting for a big blue bear like you. The Rocky Mountains indeed offer a magnificent view, and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mI'm sure they would be even more enjoyable with a taste of your favorite food, don't you think?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Of course, Beras! I now know that your favorite food is honey. It's sweet, golden, and just as delightful as the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">magnificent Rocky Mountains! Enjoying your favorite food while taking in the natural beauty of the mountains sounds</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like a perfect combination to me.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mOf course, Beras! I now know that your favorite food is honey. It's sweet, golden, and just as delightful as the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmagnificent Rocky Mountains! Enjoying your favorite food while taking in the natural beauty of the mountains sounds\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlike a perfect combination to me.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Reset knowledge base and define what it means to add more messages.\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
    "    return d.get('output')\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    "    \"\\nAnswer the user conversationally. Make sure the conversation flows naturally.\\n\"\n",
    "    \"[Agent]\"\n",
    ")\n",
    "\n",
    "\n",
    "conv_chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'input': (lambda x:x)\n",
    "    }\n",
    "    | RunnableAssign({'output' : chat_prompt | instruct_llm | StrOutputParser()})\n",
    "    | partial(save_memory_and_get_output, vstore=convstore)\n",
    ")\n",
    "\n",
    "pprint(conv_chain.invoke(\"I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Can you guess what my favorite food is?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Actually, my favorite is honey! Not sure where you got that idea?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"I see! Fair enough! Do you know my favorite food now?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KRMW6G7NVSWF",
   "metadata": {
    "id": "KRMW6G7NVSWF"
   },
   "source": [
    "不同于将上下文注入 LLM 的更自动化的全文本（full-text）或基于规则的方法，这样可避免上下文长度失控。这种策略虽然称不上完全可靠，但对于非结构化的对话来说已经是一个巨大的改进了（甚至不需要借助一个强大的指令微调模型做槽位填充）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9TPkh3SaLbqh",
   "metadata": {
    "id": "9TPkh3SaLbqh"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 3 部分 [练习]：** 用 RAG 进行文档块检索\n",
    "\n",
    "鉴于我们之前对文档加载的探索，您应该已经熟悉对数据块嵌入和检索了。现在值得花点时间继续过一遍，因为把 RAG 用在文档上是一把双刃剑：它看起来似乎开箱即用，但想让它在实际应用中保持可靠的性能需要非常谨慎地优化。我们也借此机会回顾一下基本的 LCEL 技能！\n",
    "\n",
    "<br> \n",
    "\n",
    "### **练习：**\n",
    "\n",
    "您可能还记得之前我们用 [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv) 加载了一些比较短的文章：\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "docs = [\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL\n",
    "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct\n",
    "]\n",
    "```\n",
    "\n",
    "根据所学，选择几个论文，并开发一个能讨论这些论文的聊天机器人！\n",
    "\n",
    "<br>  \n",
    "\n",
    "虽然这是一项相当艰巨的任务，但下面将提供**大部分**实现过程。演示过后，许多必须的环节就已经实现好了，您真正的任务是将它们集成到最终的 `retrieval_chain`。您会在最后一个 notebook 把它们集成到链中来完成评估测试！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jSjfCtiQnj9e",
   "metadata": {
    "id": "jSjfCtiQnj9e"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **任务 1：** 载入并分块您的文档\n",
    "\n",
    "以下代码提供了一些可以载入到 RAG 链的默认论文。您可以根据需要选更多的论文，但要注意长文档的处理时间也更长。其中还有一些利于提高 RAG 性能的简化假设及处理步骤：\n",
    "\n",
    "* 文档仅截取“参考“”（References）部分之前的内容。防止系统考虑冗长和不重要的引用和附录。\n",
    "* 有一个能提供全局视角的列出所有可用文档的数据块。如果您的工作流并不是每次检索都提供元数据，那么这个数据块就会很有用，甚至可以在合适的时候作为更高优先级信息的一部分。\n",
    "* 此外，还会插入元数据条目以提供常规信息。理想情况下，会有一些融合进了元数据的跨文档数据块。\n",
    "\n",
    "**注意：** ***为执行评估，请至少放进一篇发表时间不超过一个月的论文！***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "S-3FBdT_lhVT",
   "metadata": {
    "id": "S-3FBdT_lhVT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Documents\n",
      "Chunking Documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral 7B</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Graph RAG-Tool Fusion </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Attention Is All You Need\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msources and discrete reasoning\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Mistral 7B\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Graph RAG-Tool Fusion \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0\n",
      " - # Chunks: 35\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-02'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kaiser, Illia Polosukhin'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">constituency parsing both with large and limited training data.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-08-02'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Attention Is All You Need'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz \u001b[0m\n",
       "\u001b[32mKaiser, Illia Polosukhin'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural \u001b[0m\n",
       "\u001b[32mnetworks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder \u001b[0m\n",
       "\u001b[32mthrough an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on \u001b[0m\n",
       "\u001b[32mattention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation\u001b[0m\n",
       "\u001b[32mtasks show these models to be\\nsuperior in quality while being more parallelizable and requiring \u001b[0m\n",
       "\u001b[32msignificantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation \u001b[0m\n",
       "\u001b[32mtask, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 \u001b[0m\n",
       "\u001b[32mEnglish-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 \u001b[0m\n",
       "\u001b[32mafter training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the \u001b[0m\n",
       "\u001b[32mliterature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish \u001b[0m\n",
       "\u001b[32mconstituency parsing both with large and limited training data.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1\n",
      " - # Chunks: 45\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2019-05-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Encoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">layer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">empirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">including\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">absolute improvement), SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2019-05-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional \u001b[0m\n",
       "\u001b[32mEncoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to \u001b[0m\n",
       "\u001b[32mpre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right \u001b[0m\n",
       "\u001b[32mcontext in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output \u001b[0m\n",
       "\u001b[32mlayer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language \u001b[0m\n",
       "\u001b[32minference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and \u001b[0m\n",
       "\u001b[32mempirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, \u001b[0m\n",
       "\u001b[32mincluding\\npushing the GLUE score to 80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, MultiNLI\\naccuracy to 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% \u001b[0m\n",
       "\u001b[32mabsolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, SQuAD v1.1 question answering\\nTest F1 to 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD \u001b[0m\n",
       "\u001b[32mv2.0 Test F1 to 83.1\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 2\n",
      " - # Chunks: 46\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-04-12'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">behind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">world knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">downstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-04-12'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, \u001b[0m\n",
       "\u001b[32mHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, \u001b[0m\n",
       "\u001b[32mand achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and\u001b[0m\n",
       "\u001b[32mprecisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags \u001b[0m\n",
       "\u001b[32mbehind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their \u001b[0m\n",
       "\u001b[32mworld knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism \u001b[0m\n",
       "\u001b[32mto\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive \u001b[0m\n",
       "\u001b[32mdownstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m -- \u001b[0m\n",
       "\u001b[32mmodels which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG \u001b[0m\n",
       "\u001b[32mmodels where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector \u001b[0m\n",
       "\u001b[32mindex\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which \u001b[0m\n",
       "\u001b[32mconditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different \u001b[0m\n",
       "\u001b[32mpassages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set\u001b[0m\n",
       "\u001b[32mthe state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific \u001b[0m\n",
       "\u001b[32mretrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more \u001b[0m\n",
       "\u001b[32mspecific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 3\n",
      " - # Chunks: 40\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-05-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge sources and discrete reasoning'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Amnon Shashua, Moshe Tenenholtz'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Huge language models (LMs) have ushered in a new era for AI, serving as a\\ngateway to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">natural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">systems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge and Language (MRKL, pronounced \"miracle\") system,\\nsome of the technical challenges in implementing it, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2022-05-01'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external \u001b[0m\n",
       "\u001b[32mknowledge sources and discrete reasoning'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit\u001b[0m\n",
       "\u001b[32mBata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, \u001b[0m\n",
       "\u001b[32mAmnon Shashua, Moshe Tenenholtz'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Huge language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have ushered in a new era for AI, serving as a\\ngateway to \u001b[0m\n",
       "\u001b[32mnatural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently \u001b[0m\n",
       "\u001b[32mlimited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a \u001b[0m\n",
       "\u001b[32msystems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to \u001b[0m\n",
       "\u001b[32mlinguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete \u001b[0m\n",
       "\u001b[32mknowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, \u001b[0m\n",
       "\u001b[32mKnowledge and Language \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMRKL, pronounced \"miracle\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m system,\\nsome of the technical challenges in implementing it, \u001b[0m\n",
       "\u001b[32mand Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 4\n",
      " - # Chunks: 21\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-10-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Mistral 7B'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">under the Apache 2.0 license.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-10-10'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Mistral 7B'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
       "\u001b[32mDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, \u001b[0m\n",
       "\u001b[32mMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior \u001b[0m\n",
       "\u001b[32mperformance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in\u001b[0m\n",
       "\u001b[32mreasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for \u001b[0m\n",
       "\u001b[32mfaster\\ninference, coupled with sliding window attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to effectively handle\\nsequences of arbitrary length\u001b[0m\n",
       "\u001b[32mwith a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, \u001b[0m\n",
       "\u001b[32mthat surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released \u001b[0m\n",
       "\u001b[32munder the Apache 2.0 license.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 5\n",
      " - # Chunks: 44\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conversations with\\nhuman preferences are publicly available </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-12-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, \u001b[0m\n",
       "\u001b[32mZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Evaluating large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m based chat assistants is challenging\\ndue to their broad \u001b[0m\n",
       "\u001b[32mcapabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore\u001b[0m\n",
       "\u001b[32musing strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and \u001b[0m\n",
       "\u001b[32mlimitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited \u001b[0m\n",
       "\u001b[32mreasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between \u001b[0m\n",
       "\u001b[32mLLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot \u001b[0m\n",
       "\u001b[32mArena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both \u001b[0m\n",
       "\u001b[32mcontrolled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement \u001b[0m\n",
       "\u001b[32mbetween humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which \u001b[0m\n",
       "\u001b[32mare otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement\u001b[0m\n",
       "\u001b[32meach other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K \u001b[0m\n",
       "\u001b[32mconversations with\\nhuman preferences are publicly available \u001b[0m\n",
       "\u001b[32mat\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 6\n",
      " - # Chunks: 36\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-02-11'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Graph RAG-Tool Fusion'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Elias Lumer, Pradeep Honaganahalli Basavaraju, Myles Mason, James A. Burke, Vamse Kumar Subbiah'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Recent developments in retrieval-augmented generation (RAG) for selecting\\nrelevant tools from a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tool knowledge base enable LLM agents to scale their\\ncomplex tool calling capabilities to hundreds or thousands of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">external tools,\\nAPIs, or agents-as-tools. However, traditional RAG-based tool retrieval fails\\nto capture </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">structured dependencies between tools, limiting the retrieval\\naccuracy of a retrieved tool\\'s dependencies. For </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">example, among a vector\\ndatabase of tools, a \"get stock price\" API requires a \"stock ticker\" parameter\\nfrom a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"get stock ticker\" API, and both depend on OS-level internet\\nconnectivity tools. In this paper, we address this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limitation by introducing\\nGraph RAG-Tool Fusion, a novel plug-and-play approach that combines the\\nstrengths of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vector-based retrieval with efficient graph traversal to capture\\nall relevant tools (nodes) along with any nested </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dependencies (edges) within\\nthe predefined tool knowledge graph. We also present ToolLinkOS, a new tool\\nselection</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">benchmark of 573 fictional tools, spanning over 15 industries, each\\nwith an average of 6.3 tool dependencies. We </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">demonstrate that Graph RAG-Tool\\nFusion achieves absolute improvements of 71.7% and 22.1% over na\\\\\"ive RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on\\nToolLinkOS and ToolSandbox benchmarks, respectively (mAP@10). ToolLinkOS\\ndataset is available </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at\\nhttps://github.com/EliasLumer/Graph-RAG-Tool-Fusion-ToolLinkOS'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2025-02-11'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Graph RAG-Tool Fusion'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Elias Lumer, Pradeep Honaganahalli Basavaraju, Myles Mason, James A. Burke, Vamse Kumar Subbiah'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Recent developments in retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for selecting\\nrelevant tools from a \u001b[0m\n",
       "\u001b[32mtool knowledge base enable LLM agents to scale their\\ncomplex tool calling capabilities to hundreds or thousands of\u001b[0m\n",
       "\u001b[32mexternal tools,\\nAPIs, or agents-as-tools. However, traditional RAG-based tool retrieval fails\\nto capture \u001b[0m\n",
       "\u001b[32mstructured dependencies between tools, limiting the retrieval\\naccuracy of a retrieved tool\\'s dependencies. For \u001b[0m\n",
       "\u001b[32mexample, among a vector\\ndatabase of tools, a \"get stock price\" API requires a \"stock ticker\" parameter\\nfrom a \u001b[0m\n",
       "\u001b[32m\"get stock ticker\" API, and both depend on OS-level internet\\nconnectivity tools. In this paper, we address this \u001b[0m\n",
       "\u001b[32mlimitation by introducing\\nGraph RAG-Tool Fusion, a novel plug-and-play approach that combines the\\nstrengths of \u001b[0m\n",
       "\u001b[32mvector-based retrieval with efficient graph traversal to capture\\nall relevant tools \u001b[0m\u001b[32m(\u001b[0m\u001b[32mnodes\u001b[0m\u001b[32m)\u001b[0m\u001b[32m along with any nested \u001b[0m\n",
       "\u001b[32mdependencies \u001b[0m\u001b[32m(\u001b[0m\u001b[32medges\u001b[0m\u001b[32m)\u001b[0m\u001b[32m within\\nthe predefined tool knowledge graph. We also present ToolLinkOS, a new tool\\nselection\u001b[0m\n",
       "\u001b[32mbenchmark of 573 fictional tools, spanning over 15 industries, each\\nwith an average of 6.3 tool dependencies. We \u001b[0m\n",
       "\u001b[32mdemonstrate that Graph RAG-Tool\\nFusion achieves absolute improvements of 71.7% and 22.1% over na\\\\\"ive RAG \u001b[0m\n",
       "\u001b[32mon\\nToolLinkOS and ToolSandbox benchmarks, respectively \u001b[0m\u001b[32m(\u001b[0m\u001b[32mmAP@10\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. ToolLinkOS\\ndataset is available \u001b[0m\n",
       "\u001b[32mat\\nhttps://github.com/EliasLumer/Graph-RAG-Tool-Fusion-ToolLinkOS'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    ")\n",
    "\n",
    "## TODO: Please pick some papers and add them to the list as you'd like\n",
    "## NOTE: To re-use for the final assessment, make sure at least one paper is < 1 month old\n",
    "print(\"Loading Documents\")\n",
    "docs = [\n",
    "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
    "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
    "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
    "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
    "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
    "    ## Some longer papers\n",
    "    # ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
    "    # ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
    "    ArxivLoader(query=\"2502.07223\").load(),  ## CLIP Paper\n",
    "    ## TODO: Feel free to add more\n",
    "]\n",
    "\n",
    "## Cut the paper short if references is included.\n",
    "## This is a standard string in papers.\n",
    "for doc in docs:\n",
    "    content = json.dumps(doc[0].page_content)\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]\n",
    "\n",
    "## Split the documents and also filter out stubs (overly short chunks)\n",
    "print(\"Chunking Documents\")\n",
    "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
    "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
    "\n",
    "## Make some custom Chunks to give big-picture details\n",
    "doc_string = \"Available Documents:\"\n",
    "doc_metadata = []\n",
    "for chunks in docs_chunks:\n",
    "    metadata = getattr(chunks[0], 'metadata', {})\n",
    "    doc_string += \"\\n - \" + metadata.get('Title')\n",
    "    doc_metadata += [str(metadata)]\n",
    "\n",
    "extra_chunks = [doc_string] + doc_metadata\n",
    "\n",
    "## Printing out some summary information for reference\n",
    "pprint(doc_string, '\\n')\n",
    "for i, chunks in enumerate(docs_chunks):\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print(f\" - Metadata: \")\n",
    "    pprint(chunks[0].metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4pWU_OOnnrsT",
   "metadata": {
    "id": "4pWU_OOnnrsT"
   },
   "source": [
    "### **任务 2：** 构建文档向量存储\n",
    "\n",
    "我们现在已经有了所有组件，可以继续围绕它们创建索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "lwwmr3aptwCg",
   "metadata": {
    "id": "lwwmr3aptwCg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Vector Stores\n",
      "CPU times: user 822 ms, sys: 38.9 ms, total: 861 ms\n",
      "Wall time: 21.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Constructing Vector Stores\")\n",
    "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
    "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j39JwCKubto0",
   "metadata": {
    "id": "j39JwCKubto0"
   },
   "source": [
    "<br>\n",
    "\n",
    "接着像下面这样把索引合并为一个："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "Q7us66iPVc70",
   "metadata": {
    "id": "Q7us66iPVc70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 275 chunks\n"
     ]
    }
   ],
   "source": [
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VU_VEx2mqJUK",
   "metadata": {
    "id": "VU_VEx2mqJUK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **任务 3：[练习]** 实现 RAG 链\n",
    "\n",
    "终于，一切准备就绪，来实现 RAG 工作流吧！回顾一下，我们现在有：\n",
    "\n",
    "* 一种用向量存储从零创建对话记忆的方法（用 `default_FAISS()` 初始化）\n",
    "* 通过 `ArxivLoader` 预加载了包括文档信息的向量存储（存在 `docstore` 里）。\n",
    "\n",
    "再借助几个工具，就能集成您的链了！我们还提供了几个额外的便捷工具（`doc2str` 及 `RPrint`），您可以酌情使用。此外，一些启动提示词和结构已经定义好了。\n",
    "\n",
    "> **基于上述这些：** 实现 `retrieval_chain` 吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "-RXSrb1GcNff",
   "metadata": {
    "id": "-RXSrb1GcNff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'history'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . In one approach, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG-Sequence, the model uses the same document\\\\nto predict each target token. The second approach, RAG-Token, can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">predict each target token based\\\\non a different document. In the following, we formally introduce both models and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">then describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as the training and decoding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same retrieved document to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that\\\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation\\n[Quote from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . We refer to this decoding procedure as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, requiring many forward </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passes. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that p\\\\u03b8(y|x, zi) \\\\u22480 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">where y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run additional forward passes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">once the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \\\\u201cFast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Decoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">disjoint\\\\n100-word chunks, to make a total of 21M documents\\n[Quote from Graph RAG-Tool Fusion] \"Graph RAG-Tool </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Fusion\\\\nElias Lumer, Pradeep Honaganahalli Basavaraju, Myles Mason,\\\\nJames A. Burke and Vamse Kumar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Subbiah\\\\nPricewaterhouseCoopers, AI &amp; EmTech Teams\\\\nCorrespondence: elias.lumer@pwc\\\\nAbstract\\\\nRecent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">developments in retrieval-augmented\\\\ngeneration (RAG) for selecting relevant tools\\\\nfrom a tool knowledge base </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enable LLM agents\\\\nto scale their complex tool calling capabili-\\\\nties to hundreds or thousands of external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tools,\\\\nAPIs, or agents-as-tools. However, traditional\\\\nRAG-based tool retrieval fails to capture struc-\\\\ntured </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dependencies between tools, limiting the\\\\nretrieval accuracy of a retrieved tool\\\\u2019s depen-\\\\ndencies. For </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">example, among a vector database\\\\nof tools, a \\\\\"get stock price\\\\\" API requires a\\\\n\\\\\"stock ticker\\\\\" parameter </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">from a \\\\\"get stock\\\\nticker\\\\\" API, and both depend on OS-level inter-\\\\nnet connectivity tools\\n[Quote from Graph</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG-Tool Fusion] . Our Graph RAG-Tool Fu-\\\\nsion is a novel plug-and-play approach extending\\\\nGraph RAG from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">document retrieval to scalable\\\\ntool selection for agents, leveraging advanced RAG\\\\nstrategies for the initial </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vector retrieval.\\\\n2.2\\\\nKnowledge Graphs and LLMs\\\\nIntegrating knowledge graphs (KGs) and LLMs\\\\nenhances RAG by</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">leveraging structured relation-\\\\nships between document chunks and entities to\\\\nimprove reasoning, retrieval, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Q&amp;A, and summa-\\\\nrization tasks (Peng et al., 2024; Neo4j, 2024).\\\\nCommon approaches utilize an LLM or agent to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">de-\\\\ncompose queries, iteratively explore a knowledge\\\\ngraph, extract relevant subgraphs, and solve multi-\\\\nhop </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">queries (Sun et al., 2024; Li et al., 2024a; Jin\\\\net al., 2024; Hu et al., 2024). Combining various\\\\ndata </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">structures such as graphs, chunks, tables, or\\\\nretrievers can further tailor queries to correct docu-\\\\nment </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sources (Li et al., 2024b; Sarmah et al., 2024).\\\\nKnowledge graphs improve LLM long-term mem-\\\\nory </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(Guti\\\\u00e9rrez et al\\n'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'input'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Tell me about RAG!'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'history'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m''\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'context'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . In one approach, \u001b[0m\n",
       "\u001b[32mRAG-Sequence, the model uses the same document\\\\nto predict each target token. The second approach, RAG-Token, can \u001b[0m\n",
       "\u001b[32mpredict each target token based\\\\non a different document. In the following, we formally introduce both models and \u001b[0m\n",
       "\u001b[32mthen describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as the training and decoding \u001b[0m\n",
       "\u001b[32mprocedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same retrieved document to \u001b[0m\n",
       "\u001b[32mgenerate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable \u001b[0m\n",
       "\u001b[32mthat\\\\nis marginalized to get the seq2seq probability p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m via a top-K approximation\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from \u001b[0m\n",
       "\u001b[32mRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . We refer to this decoding procedure as \u001b[0m\n",
       "\u001b[32m\\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, requiring many forward \u001b[0m\n",
       "\u001b[32mpasses. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that p\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x, zi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \\\\u22480 \u001b[0m\n",
       "\u001b[32mwhere y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run additional forward passes \u001b[0m\n",
       "\u001b[32monce the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \\\\u201cFast \u001b[0m\n",
       "\u001b[32mDecoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all \u001b[0m\n",
       "\u001b[32mexperiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. \u001b[0m\u001b[32m[\u001b[0m\u001b[32m31\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mand\\\\nKarpukhin et al. \u001b[0m\u001b[32m[\u001b[0m\u001b[32m26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, we use the December 2018 dump. Each Wikipedia article is split into \u001b[0m\n",
       "\u001b[32mdisjoint\\\\n100-word chunks, to make a total of 21M documents\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Graph RAG-Tool Fusion\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \"Graph RAG-Tool \u001b[0m\n",
       "\u001b[32mFusion\\\\nElias Lumer, Pradeep Honaganahalli Basavaraju, Myles Mason,\\\\nJames A. Burke and Vamse Kumar \u001b[0m\n",
       "\u001b[32mSubbiah\\\\nPricewaterhouseCoopers, AI & EmTech Teams\\\\nCorrespondence: elias.lumer@pwc\\\\nAbstract\\\\nRecent \u001b[0m\n",
       "\u001b[32mdevelopments in retrieval-augmented\\\\ngeneration \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for selecting relevant tools\\\\nfrom a tool knowledge base \u001b[0m\n",
       "\u001b[32menable LLM agents\\\\nto scale their complex tool calling capabili-\\\\nties to hundreds or thousands of external \u001b[0m\n",
       "\u001b[32mtools,\\\\nAPIs, or agents-as-tools. However, traditional\\\\nRAG-based tool retrieval fails to capture struc-\\\\ntured \u001b[0m\n",
       "\u001b[32mdependencies between tools, limiting the\\\\nretrieval accuracy of a retrieved tool\\\\u2019s depen-\\\\ndencies. For \u001b[0m\n",
       "\u001b[32mexample, among a vector database\\\\nof tools, a \\\\\"get stock price\\\\\" API requires a\\\\n\\\\\"stock ticker\\\\\" parameter \u001b[0m\n",
       "\u001b[32mfrom a \\\\\"get stock\\\\nticker\\\\\" API, and both depend on OS-level inter-\\\\nnet connectivity tools\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Graph\u001b[0m\n",
       "\u001b[32mRAG-Tool Fusion\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Our Graph RAG-Tool Fu-\\\\nsion is a novel plug-and-play approach extending\\\\nGraph RAG from \u001b[0m\n",
       "\u001b[32mdocument retrieval to scalable\\\\ntool selection for agents, leveraging advanced RAG\\\\nstrategies for the initial \u001b[0m\n",
       "\u001b[32mvector retrieval.\\\\n2.2\\\\nKnowledge Graphs and LLMs\\\\nIntegrating knowledge graphs \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKGs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and LLMs\\\\nenhances RAG by\u001b[0m\n",
       "\u001b[32mleveraging structured relation-\\\\nships between document chunks and entities to\\\\nimprove reasoning, retrieval, \u001b[0m\n",
       "\u001b[32mQ&A, and summa-\\\\nrization tasks \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPeng et al., 2024; Neo4j, 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\\\nCommon approaches utilize an LLM or agent to \u001b[0m\n",
       "\u001b[32mde-\\\\ncompose queries, iteratively explore a knowledge\\\\ngraph, extract relevant subgraphs, and solve multi-\\\\nhop \u001b[0m\n",
       "\u001b[32mqueries \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSun et al., 2024; Li et al., 2024a; Jin\\\\net al., 2024; Hu et al., 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Combining various\\\\ndata \u001b[0m\n",
       "\u001b[32mstructures such as graphs, chunks, tables, or\\\\nretrievers can further tailor queries to correct docu-\\\\nment \u001b[0m\n",
       "\u001b[32msources \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLi et al., 2024b; Sarmah et al., 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\\\nKnowledge graphs improve LLM long-term mem-\\\\nory \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mGuti\\\\u00e9rrez et al\\n'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Conversation History Retrieval:\\n\\n\\n Document Retrieval:\\n[Quote from Retrieval-Augmented Generation for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge-Intensive NLP Tasks] . In one approach, RAG-Sequence, the model uses the same document\\\\nto predict each </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">target token. The second approach, RAG-Token, can predict each target token based\\\\non a different document. In the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following, we formally introduce both models and then describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the training and decoding procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieved document to generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">latent variable that\\\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation\\n[Quote from</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . We refer to this decoding procedure as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, requiring many forward </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passes. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that p\\\\u03b8(y|x, zi) \\\\u22480 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">where y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run additional forward passes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">once the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \\\\u201cFast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Decoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">disjoint\\\\n100-word chunks, to make a total of 21M documents\\n[Quote from Graph RAG-Tool Fusion] \"Graph RAG-Tool </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Fusion\\\\nElias Lumer, Pradeep Honaganahalli Basavaraju, Myles Mason,\\\\nJames A. Burke and Vamse Kumar </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Subbiah\\\\nPricewaterhouseCoopers, AI &amp; EmTech Teams\\\\nCorrespondence: elias.lumer@pwc\\\\nAbstract\\\\nRecent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">developments in retrieval-augmented\\\\ngeneration (RAG) for selecting relevant tools\\\\nfrom a tool knowledge base </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enable LLM agents\\\\nto scale their complex tool calling capabili-\\\\nties to hundreds or thousands of external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tools,\\\\nAPIs, or agents-as-tools. However, traditional\\\\nRAG-based tool retrieval fails to capture struc-\\\\ntured </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dependencies between tools, limiting the\\\\nretrieval accuracy of a retrieved tool\\\\u2019s depen-\\\\ndencies. For </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">example, among a vector database\\\\nof tools, a \\\\\"get stock price\\\\\" API requires a\\\\n\\\\\"stock ticker\\\\\" parameter </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">from a \\\\\"get stock\\\\nticker\\\\\" API, and both depend on OS-level inter-\\\\nnet connectivity tools\\n[Quote from Graph</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG-Tool Fusion] . Our Graph RAG-Tool Fu-\\\\nsion is a novel plug-and-play approach extending\\\\nGraph RAG from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">document retrieval to scalable\\\\ntool selection for agents, leveraging advanced RAG\\\\nstrategies for the initial </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vector retrieval.\\\\n2.2\\\\nKnowledge Graphs and LLMs\\\\nIntegrating knowledge graphs (KGs) and LLMs\\\\nenhances RAG by</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">leveraging structured relation-\\\\nships between document chunks and entities to\\\\nimprove reasoning, retrieval, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Q&amp;A, and summa-\\\\nrization tasks (Peng et al., 2024; Neo4j, 2024).\\\\nCommon approaches utilize an LLM or agent to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">de-\\\\ncompose queries, iteratively explore a knowledge\\\\ngraph, extract relevant subgraphs, and solve multi-\\\\nhop </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">queries (Sun et al., 2024; Li et al., 2024a; Jin\\\\net al., 2024; Hu et al., 2024). Combining various\\\\ndata </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">structures such as graphs, chunks, tables, or\\\\nretrievers can further tailor queries to correct docu-\\\\nment </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sources (Li et al., 2024b; Sarmah et al., 2024).\\\\nKnowledge graphs improve LLM long-term mem-\\\\nory </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(Guti\\\\u00e9rrez et al\\n\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conversational.)'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'You are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
       "\u001b[32mjust asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  \u001b[0m\n",
       "\u001b[32mConversation History Retrieval:\\n\\n\\n Document Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for \u001b[0m\n",
       "\u001b[32mKnowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . In one approach, RAG-Sequence, the model uses the same document\\\\nto predict each \u001b[0m\n",
       "\u001b[32mtarget token. The second approach, RAG-Token, can predict each target token based\\\\non a different document. In the\u001b[0m\n",
       "\u001b[32mfollowing, we formally introduce both models and then describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as \u001b[0m\n",
       "\u001b[32mthe training and decoding procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same \u001b[0m\n",
       "\u001b[32mretrieved document to generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single \u001b[0m\n",
       "\u001b[32mlatent variable that\\\\nis marginalized to get the seq2seq probability p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m via a top-K approximation\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from\u001b[0m\n",
       "\u001b[32mRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . We refer to this decoding procedure as \u001b[0m\n",
       "\u001b[32m\\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, requiring many forward \u001b[0m\n",
       "\u001b[32mpasses. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that p\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x, zi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \\\\u22480 \u001b[0m\n",
       "\u001b[32mwhere y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run additional forward passes \u001b[0m\n",
       "\u001b[32monce the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \\\\u201cFast \u001b[0m\n",
       "\u001b[32mDecoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all \u001b[0m\n",
       "\u001b[32mexperiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. \u001b[0m\u001b[32m[\u001b[0m\u001b[32m31\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mand\\\\nKarpukhin et al. \u001b[0m\u001b[32m[\u001b[0m\u001b[32m26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, we use the December 2018 dump. Each Wikipedia article is split into \u001b[0m\n",
       "\u001b[32mdisjoint\\\\n100-word chunks, to make a total of 21M documents\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Graph RAG-Tool Fusion\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \"Graph RAG-Tool \u001b[0m\n",
       "\u001b[32mFusion\\\\nElias Lumer, Pradeep Honaganahalli Basavaraju, Myles Mason,\\\\nJames A. Burke and Vamse Kumar \u001b[0m\n",
       "\u001b[32mSubbiah\\\\nPricewaterhouseCoopers, AI & EmTech Teams\\\\nCorrespondence: elias.lumer@pwc\\\\nAbstract\\\\nRecent \u001b[0m\n",
       "\u001b[32mdevelopments in retrieval-augmented\\\\ngeneration \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for selecting relevant tools\\\\nfrom a tool knowledge base \u001b[0m\n",
       "\u001b[32menable LLM agents\\\\nto scale their complex tool calling capabili-\\\\nties to hundreds or thousands of external \u001b[0m\n",
       "\u001b[32mtools,\\\\nAPIs, or agents-as-tools. However, traditional\\\\nRAG-based tool retrieval fails to capture struc-\\\\ntured \u001b[0m\n",
       "\u001b[32mdependencies between tools, limiting the\\\\nretrieval accuracy of a retrieved tool\\\\u2019s depen-\\\\ndencies. For \u001b[0m\n",
       "\u001b[32mexample, among a vector database\\\\nof tools, a \\\\\"get stock price\\\\\" API requires a\\\\n\\\\\"stock ticker\\\\\" parameter \u001b[0m\n",
       "\u001b[32mfrom a \\\\\"get stock\\\\nticker\\\\\" API, and both depend on OS-level inter-\\\\nnet connectivity tools\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Graph\u001b[0m\n",
       "\u001b[32mRAG-Tool Fusion\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Our Graph RAG-Tool Fu-\\\\nsion is a novel plug-and-play approach extending\\\\nGraph RAG from \u001b[0m\n",
       "\u001b[32mdocument retrieval to scalable\\\\ntool selection for agents, leveraging advanced RAG\\\\nstrategies for the initial \u001b[0m\n",
       "\u001b[32mvector retrieval.\\\\n2.2\\\\nKnowledge Graphs and LLMs\\\\nIntegrating knowledge graphs \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKGs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and LLMs\\\\nenhances RAG by\u001b[0m\n",
       "\u001b[32mleveraging structured relation-\\\\nships between document chunks and entities to\\\\nimprove reasoning, retrieval, \u001b[0m\n",
       "\u001b[32mQ&A, and summa-\\\\nrization tasks \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPeng et al., 2024; Neo4j, 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\\\nCommon approaches utilize an LLM or agent to \u001b[0m\n",
       "\u001b[32mde-\\\\ncompose queries, iteratively explore a knowledge\\\\ngraph, extract relevant subgraphs, and solve multi-\\\\nhop \u001b[0m\n",
       "\u001b[32mqueries \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSun et al., 2024; Li et al., 2024a; Jin\\\\net al., 2024; Hu et al., 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Combining various\\\\ndata \u001b[0m\n",
       "\u001b[32mstructures such as graphs, chunks, tables, or\\\\nretrievers can further tailor queries to correct docu-\\\\nment \u001b[0m\n",
       "\u001b[32msources \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLi et al., 2024b; Sarmah et al., 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\\\nKnowledge graphs improve LLM long-term mem-\\\\nory \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mGuti\\\\u00e9rrez et al\\n\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer only from retrieval. Only cite sources that are used. Make your response \u001b[0m\n",
       "\u001b[32mconversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'Tell me about RAG!'\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG, or Retrieval-Augmented Generation, is a concept in natural language processing that involves using retrieved documents to improve language generation tasks. It can be used in a variety of knowledge-intensive NLP tasks.\n",
      "\n",
      "In one approach, called RAG-Sequence, the model uses the same retrieved document to generate the entire sequence. This is done by treating the retrieved document as a single latent variable that is marginalized to get the sequence-to-sequence probability. This decoding procedure is called \"Thorough Decoding.\" For longer output sequences, an approximation called \"Fast Decoding\" can be used to avoid running many forward passes.\n",
      "\n",
      "In another approach, called RAG-Token, the model can predict each target token based on a different document. This allows for more flexibility in the documents that can be used for generation.\n",
      "\n",
      "RAG has been used in a wide range of experiments, with a single Wikipedia dump serving as the non-parametric knowledge source. Each Wikipedia article is split into disjoint 100-word chunks to make a total of 21 million documents.\n",
      "\n",
      "In addition to document retrieval, RAG has also been extended to tool selection for agents through a novel approach called Graph RAG-Tool Fusion. This approach captures structured dependencies between tools, improving the accuracy of retrieved tool dependencies.\n",
      "\n",
      "RAG has also been combined with knowledge graphs and large language models to improve reasoning, retrieval, question answering, and summarization tasks.\n",
      "\n",
      "Sources:\n",
      "\n",
      "* \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\n",
      "* \"Graph RAG-Tool Fusion\"\n",
      "* Neo4j (2024)\n",
      "* Li et al. (2024a, 2024b)\n",
      "* Sarmah et al. (2024)\n",
      "* Peng et al. (2024)\n",
      "* Sun et al. (2024)\n",
      "* Jin et al. (2024)\n",
      "* Hu et al. (2024)\n",
      "* Gutiérrez et al. (n.d.)"
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
    "# instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "stream_chain = chat_prompt| RPrint() | instruct_llm | StrOutputParser()\n",
    "\n",
    "################################################################################################\n",
    "## BEGIN TODO: Implement the retrieval chain to make your system work!\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    ## TODO: Make sure to retrieve history & context from convstore & docstore, respectively.\n",
    "    ## HINT: Our solution uses RunnableAssign, itemgetter, long_reorder, and docs2str\n",
    "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
    "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever() | long_reorder | docs2str})\n",
    "    | RPrint()\n",
    ")\n",
    "\n",
    "## END TODO\n",
    "################################################################################################\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n",
    "\n",
    "## Start of Agent Event Loop\n",
    "test_question = \"Tell me about RAG!\"  ## <- modify as desired\n",
    "\n",
    "## Before you launch your gradio interface, make sure your thing works\n",
    "for response in chat_gen(test_question, return_buffer=False):\n",
    "    print(response, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9W7sC5Z6BfqM",
   "metadata": {
    "id": "9W7sC5Z6BfqM"
   },
   "source": [
    "### **任务 4：** 与 Gradio 聊天机器人交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fMP3l7QL2JWT",
   "metadata": {
    "id": "fMP3l7QL2JWT"
   },
   "outputs": [],
   "source": [
    "# chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "# demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "# try:\n",
    "#     demo.launch(debug=True, share=True, show_api=False)\n",
    "#     demo.close()\n",
    "# except Exception as e:\n",
    "#     demo.close()\n",
    "#     print(e)\n",
    "#     raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCb3RVVfbmQ0",
   "metadata": {
    "id": "yCb3RVVfbmQ0"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 4 部分：** 保存索引以用于评估\n",
    "\n",
    "实现 RAG 链后，请参考[官方文档](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading)保存您积累出来的向量存储。最后的评估会用到！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "Y4se5wQ4Afda",
   "metadata": {
    "id": "Y4se5wQ4Afda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    }
   ],
   "source": [
    "## Save and compress your index\n",
    "docstore.save_local(\"docstore_index\")\n",
    "!tar czvf docstore_index.tgz docstore_index\n",
    "\n",
    "!rm -rf docstore_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LsI7NivbIgFw",
   "metadata": {
    "id": "LsI7NivbIgFw"
   },
   "source": [
    "如果所有内容都已正确保存，就可以执行以下代码从 `tgz` 压缩文件拿到索引了（只要安装好了 pip 环境）。当您确认这个代码单元能拿到您的索引之后，把 `docstore_index.tgz` 下载下来，下个 notebook 会用到！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "Qs8820ucIu1t",
   "metadata": {
    "id": "Qs8820ucIu1t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n",
      ". The\\ngraph index is built using a predefined schema to\\nmodel tool nodes and edges.\\n3.1\\nGraph Indexing of Tools\\nFrom a large corpus of M total tools, APIs, or\\nagents-as-tools, transform each t tool into a schema\\nmodel, characterizing it as either a regular tool or\\na core tool (node), with an optional list of any di-\\nrect or indirect dependencies (edge) to other tools\\n(regular or core)\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "!tar xzvf docstore_index.tgz\n",
    "new_db = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = new_db.similarity_search(\"Testing the index\")\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "as_3vWJGKB2F",
   "metadata": {
    "id": "as_3vWJGKB2F"
   },
   "source": [
    "----\n",
    "\n",
    "## **第 5 部分：** 总结\n",
    "\n",
    "恭喜！如果您的 RAG 链能正常运行，就继续进入 08_evaluation.ipynb 进行 **RAG 评估**吧！\n",
    "\n",
    "### <font color=\"#76b900\">**非常好！**</font>\n",
    "\n",
    "### **接下来**：\n",
    "**[可选]** 回顾 notebook 顶部的“思考问题”。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
